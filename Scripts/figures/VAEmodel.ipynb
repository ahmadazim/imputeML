{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "import scipy.sparse as sp_sparse\n",
    "import scanpy as sc\n",
    "from math import log\n",
    "from statistics import median\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "import keras.losses\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read_10x_mtx(\n",
    "    '/home/ahmadazim/data/filtered_gene_bc_matrices/hg19',  # the directory with the `.mtx` file\n",
    "    var_names='gene_symbols',                      # use gene symbols for the variable names (variables-axis index)\n",
    "    cache=True)\n",
    "\n",
    "adata.var_names_make_unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame.sparse.from_spmatrix(adata.X)\n",
    "print('Working on {} cells and {} genes'.format(*data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out genes that are not expressed in any cells\n",
    "geneSum = data.sum(axis=0)\n",
    "x = geneSum.index[geneSum == 0].tolist()\n",
    "data = data.drop(x, axis = 1)\n",
    "data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing data (using method from Rao, et al.)\n",
    "cellSum  = data.sum(axis=1)\n",
    "median_j = median(cellSum)\n",
    "npData = np.asarray(data)\n",
    "for j in range(2700):\n",
    "    cellSum_j = cellSum[j]\n",
    "    for i in range(16634):\n",
    "        npData[j,i] = log( ( (npData[j,i])/(cellSum_j) * median_j ) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataNorm = pd.DataFrame(npData)\n",
    "dataNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Variational Autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Sampling(layers.Layer): \n",
    "#     def call(self, inputs):\n",
    "#         mean, log_var = inputs\n",
    "#         return K.random_normal(tf.shape(log_var)) * K.exp(log_var / 2) + me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sampling layer\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullData = np.asarray(dataNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codings_size = 32\n",
    "\n",
    "inputs = layers.Input(shape=16634)\n",
    "z = layers.Dense(8000)(inputs)\n",
    "z = layers.Dense(4000, activation= \"relu\")(z)\n",
    "z = layers.Dense(1000, activation=\"relu\")(z)\n",
    "z = layers.Dense(256, activation=\"relu\")(z) \n",
    "\n",
    "codings_mean = layers.Dense(codings_size)(z) # μ \n",
    "codings_log_var = layers.Dense(codings_size)(z) # γ \n",
    "codings = Sampling()([codings_mean, codings_log_var]) \n",
    "\n",
    "variational_encoder = Model(\n",
    "    inputs=[inputs], outputs=[codings_mean, codings_log_var, codings])\n",
    "variational_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = layers.Input(shape=[codings_size])\n",
    "\n",
    "x = layers.Dense(256, activation=\"relu\")(decoder_inputs)\n",
    "x = layers.Dense(1000, activation=\"relu\")(x)\n",
    "x = layers.Dense(4000, activation=\"relu\")(x)\n",
    "x = layers.Dense(8000, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(16634, activation= \"relu\")(x)\n",
    "\n",
    "variational_decoder = Model(inputs=[decoder_inputs], outputs=[outputs])\n",
    "variational_decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _, _, codings = variational_encoder(inputs)\n",
    "# reconstructions = variational_decoder(codings)\n",
    "# variational_ae = Model(inputs=[inputs], outputs=[reconstructions])\n",
    "# variational_ae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the VAE as a Model with a custom train_step\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, variational_encoder, variational_decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.variational_encoder = variational_encoder\n",
    "        self.variational_decoder = variational_decoder\n",
    "\n",
    "    def train_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            _, _, codings = variational_encoder(data)\n",
    "            reconstruction = variational_decoder(codings)\n",
    "            \n",
    "            omega = tf.sign(data)  # 0 if 0, 1 if > 0\n",
    "            reconstruction_loss = tf.reduce_mean(tf.multiply(tf.pow( (data - reconstruction), 2), omega))\n",
    "    \n",
    "            kl_loss = 1 + codings_log_var - tf.square(codings_mean) - tf.exp(codings_log_var)\n",
    "            kl_loss = tf.reduce_mean(kl_loss)\n",
    "            kl_loss *= -0.5\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        \n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        tf.config.experimental_run_functions_eagerly(True)\n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"reconstruction_loss\": reconstruction_loss,\n",
    "            \"kl_loss\": kl_loss,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variational_ae = VAE(variational_encoder, variational_decoder)\n",
    "variational_ae.compile(optimizer= 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "VAEresults = variational_ae.fit(fullData, \n",
    "                                epochs= 10, \n",
    "                                batch_size= 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this approach didn't work (too large of a size?). Trying a different approach..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder (again)\n",
    "http://louistiao.me/posts/implementing-variational-autoencoders-in-keras-beyond-the-quickstart-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "fullData = np.asarray(dataNorm)[:,:1000]\n",
    "original_dim = 1000\n",
    "latent_dim = 16\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "epsilon_std = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nzMSE(y_true, y_pred):\n",
    "    \"\"\" MSE for nonzero values. \"\"\"\n",
    "    omega = tf.sign(y_true)  # 0 if 0, 1 if > 0\n",
    "    mse_nz = tf.reduce_mean(tf.multiply(tf.pow( (y_pred - y_true), 2), omega))\n",
    "    return mse_nz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KLDivergenceLayer(layers.Layer):\n",
    "\n",
    "    \"\"\" Identity transform layer that adds KL divergence\n",
    "    to the final model loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(KLDivergenceLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        mu, log_var = inputs\n",
    "\n",
    "        kl_batch = - .5 * K.sum(1 + log_var -\n",
    "                                K.square(mu) -\n",
    "                                K.exp(log_var), axis=-1)\n",
    "\n",
    "        self.add_loss(K.mean(kl_batch), inputs=inputs)\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Decoder \n",
    "decoder = Sequential([\n",
    "    layers.Dense(128, input_dim=latent_dim, activation='relu'),\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dense(original_dim, activation= \"relu\")\n",
    "])\n",
    "\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = layers.Input(shape= (original_dim,))\n",
    "xh = layers.Dense(512, activation=\"relu\")(x)\n",
    "h = layers.Dense(128, activation=\"relu\")(xh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mu = layers.Dense(latent_dim)(h)\n",
    "z_log_var = layers.Dense(latent_dim)(h)\n",
    "\n",
    "z_mu, z_log_var = KLDivergenceLayer()([z_mu, z_log_var])\n",
    "z_sigma = layers.Lambda(lambda t: K.exp(.5*t))(z_log_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = layers.Input(tensor=K.random_normal(stddev=epsilon_std,\n",
    "                                          shape=(K.shape(x)[0], latent_dim)))\n",
    "z_eps = layers.Multiply()([z_sigma, eps])\n",
    "z = layers.Add()([z_mu, z_eps])\n",
    "\n",
    "x_pred = decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = Model(inputs=[x, eps], outputs=x_pred)\n",
    "vae.compile(optimizer= 'adam', loss= nzMSE)\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.fit(fullData,\n",
    "        fullData,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size)"
   ]
  }
 ]
}