{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "import scanpy as sc\n",
    "from math import log\n",
    "from statistics import median\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "import keras.losses\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read_10x_mtx(\n",
    "    '/Volumes/Samsung_T5/ResearchData/scanpyTutorial/data/filtered_gene_bc_matrices/hg19',  # the directory with the `.mtx` file\n",
    "    var_names='gene_symbols',                      # use gene symbols for the variable names (variables-axis index)\n",
    "    cache=True)\n",
    "\n",
    "adata.var_names_make_unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Working on 2700 cells and 32738 genes\n"
    }
   ],
   "source": [
    "data = pd.DataFrame.sparse.from_spmatrix(adata.X)\n",
    "print('Working on {} cells and {} genes'.format(*data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(2700, 16634)"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# Filter out genes that are not expressed in any cells\n",
    "geneSum = data.sum(axis=0)\n",
    "x = geneSum.index[geneSum == 0].tolist()\n",
    "data = data.drop(x, axis = 1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing data (using method from Rao, et al.)\n",
    "cellSum  = data.sum(axis=1)\n",
    "median_j = median(cellSum)\n",
    "npData = np.asarray(data)\n",
    "for j in range(2700):\n",
    "    cellSum_j = cellSum[j]\n",
    "    for i in range(16634):\n",
    "        npData[j,i] = log( ( (npData[j,i])/(cellSum_j) * median_j ) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "      0      1      2      3      4      5      6      7      8      9      \\\n0       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n1       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n2       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n3       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n4       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n...     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n2695    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n2696    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n2697    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n2698    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n2699    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n\n      ...  16624     16625  16626  16627     16628  16629     16630  16631  \\\n0     ...    0.0  1.532536    0.0    0.0  0.000000    0.0  0.000000    0.0   \n1     ...    0.0  1.522734    0.0    0.0  0.370248    0.0  0.000000    0.0   \n2     ...    0.0  1.332558    0.0    0.0  0.000000    0.0  0.000000    0.0   \n3     ...    0.0  0.980213    0.0    0.0  0.000000    0.0  0.000000    0.0   \n4     ...    0.0  1.175435    0.0    0.0  0.000000    0.0  0.000000    0.0   \n...   ...    ...       ...    ...    ...       ...    ...       ...    ...   \n2695  ...    0.0  0.491513    0.0    0.0  0.000000    0.0  0.000000    0.0   \n2696  ...    0.0  1.266796    0.0    0.0  0.000000    0.0  0.000000    0.0   \n2697  ...    0.0  1.827533    0.0    0.0  0.000000    0.0  0.000000    0.0   \n2698  ...    0.0  1.145975    0.0    0.0  0.000000    0.0  1.145975    0.0   \n2699  ...    0.0  1.463349    0.0    0.0  0.000000    0.0  0.000000    0.0   \n\n      16632  16633  \n0       0.0    0.0  \n1       0.0    0.0  \n2       0.0    0.0  \n3       0.0    0.0  \n4       0.0    0.0  \n...     ...    ...  \n2695    0.0    0.0  \n2696    0.0    0.0  \n2697    0.0    0.0  \n2698    0.0    0.0  \n2699    0.0    0.0  \n\n[2700 rows x 16634 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>16624</th>\n      <th>16625</th>\n      <th>16626</th>\n      <th>16627</th>\n      <th>16628</th>\n      <th>16629</th>\n      <th>16630</th>\n      <th>16631</th>\n      <th>16632</th>\n      <th>16633</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.532536</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.522734</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.370248</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.332558</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.980213</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.175435</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2695</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.491513</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2696</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.266796</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2697</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.827533</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2698</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.145975</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>1.145975</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2699</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.463349</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2700 rows × 16634 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "dataNorm = pd.DataFrame(npData)\n",
    "dataNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Autoencoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Info from TF tutorial below, but model not actually implemented)\n",
    "- Use 2 small ConvNets for the encoder and decoder networks (inference/recognition and generative models)\n",
    "- *x* = observation and *z* = latent variable\n",
    "- Encoder network defines the approximate posterior distribution *q(z|x)*\n",
    "    - Use 2 convolutional layers followed by a fully-connected layer\n",
    "- Decoder network defines the approximate conditional distribution *p(x|z)*\n",
    "    - Use a fully-connected layer followed by three convolution transpose layers\n",
    "- Reparameterization Trick...\n",
    "    - Backpropagation cannot flow through random node\n",
    "    - Approximate *z* using the decoder parameters and another parameter $\\epsilon$ as follows: \n",
    "        - $z = \\mu + \\sigma \\odot \\epsilon$\n",
    "        - where $\\mu$ and $\\sigma$ represent the mean and standard deviation of a Gaussian distribution respectively\n",
    "    - Enables model to backpropagate gradients in the encoder through $\\mu$ and $\\sigma$ while maintaining stochasticity through $\\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational AutoEncoder (keras tutorial, but 1 Dimensional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going to try dividing matrix into small \"images\" \n",
    "(2700 x 16634) --> 225 (12 x 72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(2700, 16200)"
     },
     "metadata": {},
     "execution_count": 195
    }
   ],
   "source": [
    "VMR = dataNorm.std() / dataNorm.mean() \n",
    "lowestVMR = VMR.sort_values(ascending=False)[(225*72):]\n",
    "fullData = np.asarray(dataNorm.drop(lowestVMR.index, axis= 1))\n",
    "fullData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Starting i = 1.0\nStarting i = 2.0\nStarting i = 3.0\nStarting i = 4.0\nStarting i = 5.0\nStarting i = 6.0\nStarting i = 7.0\nStarting i = 8.0\nStarting i = 9.0\nStarting i = 10.0\nStarting i = 11.0\nStarting i = 12.0\nStarting i = 13.0\nStarting i = 14.0\nStarting i = 15.0\nStarting i = 16.0\nStarting i = 17.0\nStarting i = 18.0\nStarting i = 19.0\nStarting i = 20.0\nStarting i = 21.0\nStarting i = 22.0\nStarting i = 23.0\nStarting i = 24.0\nStarting i = 25.0\nStarting i = 26.0\nStarting i = 27.0\nStarting i = 28.0\nStarting i = 29.0\nStarting i = 30.0\nStarting i = 31.0\nStarting i = 32.0\nStarting i = 33.0\nStarting i = 34.0\nStarting i = 35.0\nStarting i = 36.0\nStarting i = 37.0\nStarting i = 38.0\nStarting i = 39.0\nStarting i = 40.0\nStarting i = 41.0\nStarting i = 42.0\nStarting i = 43.0\nStarting i = 44.0\nStarting i = 45.0\nStarting i = 46.0\nStarting i = 47.0\nStarting i = 48.0\nStarting i = 49.0\nStarting i = 50.0\nStarting i = 51.0\nStarting i = 52.0\nStarting i = 53.0\nStarting i = 54.0\nStarting i = 55.0\nStarting i = 56.0\nStarting i = 57.0\nStarting i = 58.0\nStarting i = 59.0\nStarting i = 60.0\nStarting i = 61.0\nStarting i = 62.0\nStarting i = 63.0\nStarting i = 64.0\nStarting i = 65.0\nStarting i = 66.0\nStarting i = 67.0\nStarting i = 68.0\nStarting i = 69.0\nStarting i = 70.0\nStarting i = 71.0\nStarting i = 72.0\nStarting i = 73.0\nStarting i = 74.0\nStarting i = 75.0\nStarting i = 76.0\nStarting i = 77.0\nStarting i = 78.0\nStarting i = 79.0\nStarting i = 80.0\nStarting i = 81.0\nStarting i = 82.0\nStarting i = 83.0\nStarting i = 84.0\nStarting i = 85.0\nStarting i = 86.0\nStarting i = 87.0\nStarting i = 88.0\nStarting i = 89.0\nStarting i = 90.0\nStarting i = 91.0\nStarting i = 92.0\nStarting i = 93.0\nStarting i = 94.0\nStarting i = 95.0\nStarting i = 96.0\nStarting i = 97.0\nStarting i = 98.0\nStarting i = 99.0\nStarting i = 100.0\nStarting i = 101.0\nStarting i = 102.0\nStarting i = 103.0\nStarting i = 104.0\nStarting i = 105.0\nStarting i = 106.0\nStarting i = 107.0\nStarting i = 108.0\nStarting i = 109.0\nStarting i = 110.0\nStarting i = 111.0\nStarting i = 112.0\nStarting i = 113.0\nStarting i = 114.0\nStarting i = 115.0\nStarting i = 116.0\nStarting i = 117.0\nStarting i = 118.0\nStarting i = 119.0\nStarting i = 120.0\nStarting i = 121.0\nStarting i = 122.0\nStarting i = 123.0\nStarting i = 124.0\nStarting i = 125.0\nStarting i = 126.0\nStarting i = 127.0\nStarting i = 128.0\nStarting i = 129.0\nStarting i = 130.0\nStarting i = 131.0\nStarting i = 132.0\nStarting i = 133.0\nStarting i = 134.0\nStarting i = 135.0\nStarting i = 136.0\nStarting i = 137.0\nStarting i = 138.0\nStarting i = 139.0\nStarting i = 140.0\nStarting i = 141.0\nStarting i = 142.0\nStarting i = 143.0\nStarting i = 144.0\nStarting i = 145.0\nStarting i = 146.0\nStarting i = 147.0\nStarting i = 148.0\nStarting i = 149.0\nStarting i = 150.0\nStarting i = 151.0\nStarting i = 152.0\nStarting i = 153.0\nStarting i = 154.0\nStarting i = 155.0\nStarting i = 156.0\nStarting i = 157.0\nStarting i = 158.0\nStarting i = 159.0\nStarting i = 160.0\nStarting i = 161.0\nStarting i = 162.0\nStarting i = 163.0\nStarting i = 164.0\nStarting i = 165.0\nStarting i = 166.0\nStarting i = 167.0\nStarting i = 168.0\nStarting i = 169.0\nStarting i = 170.0\nStarting i = 171.0\nStarting i = 172.0\nStarting i = 173.0\nStarting i = 174.0\nStarting i = 175.0\nStarting i = 176.0\nStarting i = 177.0\nStarting i = 178.0\nStarting i = 179.0\nStarting i = 180.0\nStarting i = 181.0\nStarting i = 182.0\nStarting i = 183.0\nStarting i = 184.0\nStarting i = 185.0\nStarting i = 186.0\nStarting i = 187.0\nStarting i = 188.0\nStarting i = 189.0\nStarting i = 190.0\nStarting i = 191.0\nStarting i = 192.0\nStarting i = 193.0\nStarting i = 194.0\nStarting i = 195.0\nStarting i = 196.0\nStarting i = 197.0\nStarting i = 198.0\nStarting i = 199.0\nStarting i = 200.0\nStarting i = 201.0\nStarting i = 202.0\nStarting i = 203.0\nStarting i = 204.0\nStarting i = 205.0\nStarting i = 206.0\nStarting i = 207.0\nStarting i = 208.0\nStarting i = 209.0\nStarting i = 210.0\nStarting i = 211.0\nStarting i = 212.0\nStarting i = 213.0\nStarting i = 214.0\nStarting i = 215.0\nStarting i = 216.0\nStarting i = 217.0\nStarting i = 218.0\nStarting i = 219.0\nStarting i = 220.0\nStarting i = 221.0\nStarting i = 222.0\nStarting i = 223.0\nStarting i = 224.0\nStarting i = 225.0\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(50625, 12, 72, 1)"
     },
     "metadata": {},
     "execution_count": 241
    }
   ],
   "source": [
    "# \"Chunking\" the matrix \n",
    "reshapeData = np.array([[[0 for x in range(72)] for x in range(12)] for x in range(50625)], dtype = 'float32')\n",
    "for i in range(0, 2700, 12):   # i refers to cells (12)\n",
    "    print(\"Starting i =\", (i/12)+1)\n",
    "    for j in range(0, 16200, 72):   # j refers to genes (72)\n",
    "        x = ( 225*(i/12) ) + ( (j/72)+1 )\n",
    "        smallMat = fullData[ i:(i+12) , j:(j+72) ]\n",
    "        reshapeData[int(x-1),:,:] = smallMat\n",
    "\n",
    "reshapeData = reshapeData.reshape((reshapeData.shape[0], reshapeData.shape[1], reshapeData.shape[2], 1))\n",
    "reshapeData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sampling layer\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"encoder\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_16 (InputLayer)           [(None, 12, 72, 1)]  0                                            \n__________________________________________________________________________________________________\nconv2d_14 (Conv2D)              (None, 6, 36, 32)    320         input_16[0][0]                   \n__________________________________________________________________________________________________\nconv2d_15 (Conv2D)              (None, 3, 18, 64)    18496       conv2d_14[0][0]                  \n__________________________________________________________________________________________________\nflatten_8 (Flatten)             (None, 3456)         0           conv2d_15[0][0]                  \n__________________________________________________________________________________________________\ndense_15 (Dense)                (None, 16)           55312       flatten_8[0][0]                  \n__________________________________________________________________________________________________\nz_mean (Dense)                  (None, 32)           544         dense_15[0][0]                   \n__________________________________________________________________________________________________\nz_log_var (Dense)               (None, 32)           544         dense_15[0][0]                   \n__________________________________________________________________________________________________\nsampling_8 (Sampling)           (None, 32)           0           z_mean[0][0]                     \n                                                                 z_log_var[0][0]                  \n==================================================================================================\nTotal params: 75,216\nTrainable params: 75,216\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
    }
   ],
   "source": [
    "# Build the encoder\n",
    "latent_dim = 32\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(12, 72, 1))\n",
    "x = layers.Conv2D(filters= 32, kernel_size= 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "x = layers.Conv2D(filters= 64, kernel_size= 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"decoder\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_18 (InputLayer)        [(None, 32)]              0         \n_________________________________________________________________\ndense_17 (Dense)             (None, 3456)              114048    \n_________________________________________________________________\nreshape_8 (Reshape)          (None, 3, 18, 64)         0         \n_________________________________________________________________\nconv2d_transpose_21 (Conv2DT (None, 6, 36, 64)         36928     \n_________________________________________________________________\nconv2d_transpose_22 (Conv2DT (None, 12, 72, 32)        18464     \n_________________________________________________________________\nconv2d_transpose_23 (Conv2DT (None, 12, 72, 1)         289       \n=================================================================\nTotal params: 169,729\nTrainable params: 169,729\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "# Build the decoder\n",
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(3 * 18 * 64, activation=\"relu\")(latent_inputs)\n",
    "x = layers.Reshape((3, 18, 64))(x)\n",
    "x = layers.Conv2DTranspose(filters= 64, kernel_size= 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Conv2DTranspose(filters= 32, kernel_size= 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the VAE as a Model with a custom train_step\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def train_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = encoder(data)\n",
    "            reconstruction = decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                keras.losses.mean_squared_error(data, reconstruction)\n",
    "            )\n",
    "            reconstruction_loss *= 12 * 72\n",
    "            kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "            kl_loss = tf.reduce_mean(kl_loss)\n",
    "            kl_loss *= -0.5\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"reconstruction_loss\": reconstruction_loss,\n",
    "            \"kl_loss\": kl_loss,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/50\n792/792 [==============================] - 32s 41ms/step - loss: 18.0740 - reconstruction_loss: 17.6169 - kl_loss: 0.4571\nEpoch 2/50\n792/792 [==============================] - 34s 43ms/step - loss: 17.6948 - reconstruction_loss: 17.3001 - kl_loss: 0.3946\nEpoch 3/50\n792/792 [==============================] - 36s 45ms/step - loss: 17.5447 - reconstruction_loss: 17.1862 - kl_loss: 0.3585\nEpoch 4/50\n792/792 [==============================] - 40s 51ms/step - loss: 17.4276 - reconstruction_loss: 17.1040 - kl_loss: 0.3236\nEpoch 5/50\n792/792 [==============================] - 36s 46ms/step - loss: 17.3525 - reconstruction_loss: 17.0408 - kl_loss: 0.3117\nEpoch 6/50\n792/792 [==============================] - 36s 45ms/step - loss: 17.3350 - reconstruction_loss: 17.0348 - kl_loss: 0.3002\nEpoch 7/50\n792/792 [==============================] - 37s 47ms/step - loss: 17.3511 - reconstruction_loss: 17.0646 - kl_loss: 0.2865\nEpoch 8/50\n792/792 [==============================] - 34s 43ms/step - loss: 17.3766 - reconstruction_loss: 17.1070 - kl_loss: 0.2696\nEpoch 9/50\n792/792 [==============================] - 36s 45ms/step - loss: 17.2904 - reconstruction_loss: 17.0410 - kl_loss: 0.2494\nEpoch 10/50\n792/792 [==============================] - 36s 46ms/step - loss: 17.2630 - reconstruction_loss: 17.0376 - kl_loss: 0.2254\nEpoch 11/50\n792/792 [==============================] - 37s 47ms/step - loss: 17.2469 - reconstruction_loss: 17.0499 - kl_loss: 0.1971\nEpoch 12/50\n792/792 [==============================] - 36s 45ms/step - loss: 17.2295 - reconstruction_loss: 17.0652 - kl_loss: 0.1643\nEpoch 13/50\n792/792 [==============================] - 33s 42ms/step - loss: 17.2013 - reconstruction_loss: 17.0715 - kl_loss: 0.1298\nEpoch 14/50\n792/792 [==============================] - 36s 45ms/step - loss: 17.1901 - reconstruction_loss: 17.0920 - kl_loss: 0.0981\nEpoch 15/50\n792/792 [==============================] - 36s 46ms/step - loss: 17.1452 - reconstruction_loss: 17.0632 - kl_loss: 0.0819\nEpoch 16/50\n792/792 [==============================] - 37s 46ms/step - loss: 17.1347 - reconstruction_loss: 17.0553 - kl_loss: 0.0793\nEpoch 17/50\n792/792 [==============================] - 37s 46ms/step - loss: 17.1084 - reconstruction_loss: 17.0344 - kl_loss: 0.0740\nEpoch 18/50\n792/792 [==============================] - 36s 45ms/step - loss: 17.1076 - reconstruction_loss: 17.0416 - kl_loss: 0.0660\nEpoch 19/50\n792/792 [==============================] - 37s 47ms/step - loss: 17.1282 - reconstruction_loss: 17.0724 - kl_loss: 0.0557\nEpoch 20/50\n792/792 [==============================] - 37s 46ms/step - loss: 17.0979 - reconstruction_loss: 17.0543 - kl_loss: 0.0436\nEpoch 21/50\n792/792 [==============================] - 36s 46ms/step - loss: 17.1032 - reconstruction_loss: 17.0673 - kl_loss: 0.0359\nEpoch 22/50\n792/792 [==============================] - 37s 47ms/step - loss: 17.0987 - reconstruction_loss: 17.0683 - kl_loss: 0.0304\nEpoch 23/50\n792/792 [==============================] - 36s 46ms/step - loss: 17.0646 - reconstruction_loss: 17.0366 - kl_loss: 0.0280\nEpoch 24/50\n792/792 [==============================] - 37s 47ms/step - loss: 17.0523 - reconstruction_loss: 17.0241 - kl_loss: 0.0282\nEpoch 25/50\n792/792 [==============================] - 37s 47ms/step - loss: 17.0751 - reconstruction_loss: 17.0477 - kl_loss: 0.0274\nEpoch 26/50\n792/792 [==============================] - 36s 46ms/step - loss: 17.0521 - reconstruction_loss: 17.0258 - kl_loss: 0.0263\nEpoch 27/50\n792/792 [==============================] - 38s 47ms/step - loss: 17.0749 - reconstruction_loss: 17.0499 - kl_loss: 0.0250\nEpoch 28/50\n792/792 [==============================] - 37s 46ms/step - loss: 17.0747 - reconstruction_loss: 17.0515 - kl_loss: 0.0233\nEpoch 29/50\n792/792 [==============================] - 37s 46ms/step - loss: 17.0418 - reconstruction_loss: 17.0205 - kl_loss: 0.0214\nEpoch 30/50\n792/792 [==============================] - 38s 47ms/step - loss: 17.0452 - reconstruction_loss: 17.0257 - kl_loss: 0.0195\nEpoch 31/50\n792/792 [==============================] - 37s 46ms/step - loss: 17.0733 - reconstruction_loss: 17.0561 - kl_loss: 0.0172\nEpoch 32/50\n792/792 [==============================] - 38s 48ms/step - loss: 17.0613 - reconstruction_loss: 17.0467 - kl_loss: 0.0146\nEpoch 33/50\n792/792 [==============================] - 38s 47ms/step - loss: 17.0360 - reconstruction_loss: 17.0218 - kl_loss: 0.0142\nEpoch 34/50\n792/792 [==============================] - 36s 46ms/step - loss: 17.0593 - reconstruction_loss: 17.0476 - kl_loss: 0.0117\nEpoch 35/50\n792/792 [==============================] - 37s 47ms/step - loss: 17.0665 - reconstruction_loss: 17.0568 - kl_loss: 0.0097\nEpoch 36/50\n792/792 [==============================] - 36s 45ms/step - loss: 17.0552 - reconstruction_loss: 17.0478 - kl_loss: 0.0074\nEpoch 37/50\n792/792 [==============================] - 37s 46ms/step - loss: 17.0322 - reconstruction_loss: 17.0264 - kl_loss: 0.0058\nEpoch 38/50\n792/792 [==============================] - 36s 46ms/step - loss: 17.0767 - reconstruction_loss: 17.0724 - kl_loss: 0.0043\nEpoch 39/50\n792/792 [==============================] - 36s 45ms/step - loss: 17.0557 - reconstruction_loss: 17.0523 - kl_loss: 0.0034\nEpoch 40/50\n792/792 [==============================] - 36s 46ms/step - loss: 17.0359 - reconstruction_loss: 17.0332 - kl_loss: 0.0027\nEpoch 41/50\n792/792 [==============================] - 36s 45ms/step - loss: 17.0418 - reconstruction_loss: 17.0398 - kl_loss: 0.0021\nEpoch 42/50\n792/792 [==============================] - 36s 45ms/step - loss: 17.0249 - reconstruction_loss: 17.0237 - kl_loss: 0.0011\nEpoch 43/50\n792/792 [==============================] - 35s 45ms/step - loss: 17.0653 - reconstruction_loss: 17.0645 - kl_loss: 7.3957e-04\nEpoch 44/50\n792/792 [==============================] - 35s 45ms/step - loss: 17.0401 - reconstruction_loss: 17.0398 - kl_loss: 2.5749e-04\nEpoch 45/50\n792/792 [==============================] - 35s 44ms/step - loss: 17.0298 - reconstruction_loss: 17.0297 - kl_loss: 4.2766e-05\nEpoch 46/50\n792/792 [==============================] - 35s 44ms/step - loss: 17.0346 - reconstruction_loss: 17.0346 - kl_loss: 3.4714e-06\nEpoch 47/50\n792/792 [==============================] - 35s 44ms/step - loss: 17.0290 - reconstruction_loss: 17.0290 - kl_loss: 9.4074e-08\nEpoch 48/50\n792/792 [==============================] - 35s 44ms/step - loss: 17.0684 - reconstruction_loss: 17.0684 - kl_loss: 4.5803e-10\nEpoch 49/50\n792/792 [==============================] - 36s 45ms/step - loss: 17.0336 - reconstruction_loss: 17.0336 - kl_loss: 0.0000e+00\nEpoch 50/50\n792/792 [==============================] - 35s 44ms/step - loss: 17.0312 - reconstruction_loss: 17.0312 - kl_loss: 0.0000e+00\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7fbaddac2130>"
     },
     "metadata": {},
     "execution_count": 287
    }
   ],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())\n",
    "vae.fit(reshapeData, epochs=50, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encDec = decoder.predict(encoder.predict(reshapeData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(50625, 12, 72, 1)"
     },
     "metadata": {},
     "execution_count": 302
    }
   ],
   "source": [
    "encDec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Starting run = 100.0\nStarting run = 200.0\nStarting run = 300.0\nStarting run = 400.0\nStarting run = 500.0\nStarting run = 600.0\nStarting run = 700.0\nStarting run = 800.0\nStarting run = 900.0\nStarting run = 1000.0\nStarting run = 1100.0\nStarting run = 1200.0\nStarting run = 1300.0\nStarting run = 1400.0\nStarting run = 1500.0\nStarting run = 1600.0\nStarting run = 1700.0\nStarting run = 1800.0\nStarting run = 1900.0\nStarting run = 2000.0\nStarting run = 2100.0\nStarting run = 2200.0\nStarting run = 2300.0\nStarting run = 2400.0\nStarting run = 2500.0\nStarting run = 2600.0\nStarting run = 2700.0\nStarting run = 2800.0\nStarting run = 2900.0\nStarting run = 3000.0\nStarting run = 3100.0\nStarting run = 3200.0\nStarting run = 3300.0\nStarting run = 3400.0\nStarting run = 3500.0\nStarting run = 3600.0\nStarting run = 3700.0\nStarting run = 3800.0\nStarting run = 3900.0\nStarting run = 4000.0\nStarting run = 4100.0\nStarting run = 4200.0\nStarting run = 4300.0\nStarting run = 4400.0\nStarting run = 4500.0\nStarting run = 4600.0\nStarting run = 4700.0\nStarting run = 4800.0\nStarting run = 4900.0\nStarting run = 5000.0\nStarting run = 5100.0\nStarting run = 5200.0\nStarting run = 5300.0\nStarting run = 5400.0\nStarting run = 5500.0\nStarting run = 5600.0\nStarting run = 5700.0\nStarting run = 5800.0\nStarting run = 5900.0\nStarting run = 6000.0\nStarting run = 6100.0\nStarting run = 6200.0\nStarting run = 6300.0\nStarting run = 6400.0\nStarting run = 6500.0\nStarting run = 6600.0\nStarting run = 6700.0\nStarting run = 6800.0\nStarting run = 6900.0\nStarting run = 7000.0\nStarting run = 7100.0\nStarting run = 7200.0\nStarting run = 7300.0\nStarting run = 7400.0\nStarting run = 7500.0\nStarting run = 7600.0\nStarting run = 7700.0\nStarting run = 7800.0\nStarting run = 7900.0\nStarting run = 8000.0\nStarting run = 8100.0\nStarting run = 8200.0\nStarting run = 8300.0\nStarting run = 8400.0\nStarting run = 8500.0\nStarting run = 8600.0\nStarting run = 8700.0\nStarting run = 8800.0\nStarting run = 8900.0\nStarting run = 9000.0\nStarting run = 9100.0\nStarting run = 9200.0\nStarting run = 9300.0\nStarting run = 9400.0\nStarting run = 9500.0\nStarting run = 9600.0\nStarting run = 9700.0\nStarting run = 9800.0\nStarting run = 9900.0\nStarting run = 10000.0\nStarting run = 10100.0\nStarting run = 10200.0\nStarting run = 10300.0\nStarting run = 10400.0\nStarting run = 10500.0\nStarting run = 10600.0\nStarting run = 10700.0\nStarting run = 10800.0\nStarting run = 10900.0\nStarting run = 11000.0\nStarting run = 11100.0\nStarting run = 11200.0\nStarting run = 11300.0\nStarting run = 11400.0\nStarting run = 11500.0\nStarting run = 11600.0\nStarting run = 11700.0\nStarting run = 11800.0\nStarting run = 11900.0\nStarting run = 12000.0\nStarting run = 12100.0\nStarting run = 12200.0\nStarting run = 12300.0\nStarting run = 12400.0\nStarting run = 12500.0\nStarting run = 12600.0\nStarting run = 12700.0\nStarting run = 12800.0\nStarting run = 12900.0\nStarting run = 13000.0\nStarting run = 13100.0\nStarting run = 13200.0\nStarting run = 13300.0\nStarting run = 13400.0\nStarting run = 13500.0\nStarting run = 13600.0\nStarting run = 13700.0\nStarting run = 13800.0\nStarting run = 13900.0\nStarting run = 14000.0\nStarting run = 14100.0\nStarting run = 14200.0\nStarting run = 14300.0\nStarting run = 14400.0\nStarting run = 14500.0\nStarting run = 14600.0\nStarting run = 14700.0\nStarting run = 14800.0\nStarting run = 14900.0\nStarting run = 15000.0\nStarting run = 15100.0\nStarting run = 15200.0\nStarting run = 15300.0\nStarting run = 15400.0\nStarting run = 15500.0\nStarting run = 15600.0\nStarting run = 15700.0\nStarting run = 15800.0\nStarting run = 15900.0\nStarting run = 16000.0\nStarting run = 16100.0\nStarting run = 16200.0\nStarting run = 16300.0\nStarting run = 16400.0\nStarting run = 16500.0\nStarting run = 16600.0\nStarting run = 16700.0\nStarting run = 16800.0\nStarting run = 16900.0\nStarting run = 17000.0\nStarting run = 17100.0\nStarting run = 17200.0\nStarting run = 17300.0\nStarting run = 17400.0\nStarting run = 17500.0\nStarting run = 17600.0\nStarting run = 17700.0\nStarting run = 17800.0\nStarting run = 17900.0\nStarting run = 18000.0\nStarting run = 18100.0\nStarting run = 18200.0\nStarting run = 18300.0\nStarting run = 18400.0\nStarting run = 18500.0\nStarting run = 18600.0\nStarting run = 18700.0\nStarting run = 18800.0\nStarting run = 18900.0\nStarting run = 19000.0\nStarting run = 19100.0\nStarting run = 19200.0\nStarting run = 19300.0\nStarting run = 19400.0\nStarting run = 19500.0\nStarting run = 19600.0\nStarting run = 19700.0\nStarting run = 19800.0\nStarting run = 19900.0\nStarting run = 20000.0\nStarting run = 20100.0\nStarting run = 20200.0\nStarting run = 20300.0\nStarting run = 20400.0\nStarting run = 20500.0\nStarting run = 20600.0\nStarting run = 20700.0\nStarting run = 20800.0\nStarting run = 20900.0\nStarting run = 21000.0\nStarting run = 21100.0\nStarting run = 21200.0\nStarting run = 21300.0\nStarting run = 21400.0\nStarting run = 21500.0\nStarting run = 21600.0\nStarting run = 21700.0\nStarting run = 21800.0\nStarting run = 21900.0\nStarting run = 22000.0\nStarting run = 22100.0\nStarting run = 22200.0\nStarting run = 22300.0\nStarting run = 22400.0\nStarting run = 22500.0\nStarting run = 22600.0\nStarting run = 22700.0\nStarting run = 22800.0\nStarting run = 22900.0\nStarting run = 23000.0\nStarting run = 23100.0\nStarting run = 23200.0\nStarting run = 23300.0\nStarting run = 23400.0\nStarting run = 23500.0\nStarting run = 23600.0\nStarting run = 23700.0\nStarting run = 23800.0\nStarting run = 23900.0\nStarting run = 24000.0\nStarting run = 24100.0\nStarting run = 24200.0\nStarting run = 24300.0\nStarting run = 24400.0\nStarting run = 24500.0\nStarting run = 24600.0\nStarting run = 24700.0\nStarting run = 24800.0\nStarting run = 24900.0\nStarting run = 25000.0\nStarting run = 25100.0\nStarting run = 25200.0\nStarting run = 25300.0\nStarting run = 25400.0\nStarting run = 25500.0\nStarting run = 25600.0\nStarting run = 25700.0\nStarting run = 25800.0\nStarting run = 25900.0\nStarting run = 26000.0\nStarting run = 26100.0\nStarting run = 26200.0\nStarting run = 26300.0\nStarting run = 26400.0\nStarting run = 26500.0\nStarting run = 26600.0\nStarting run = 26700.0\nStarting run = 26800.0\nStarting run = 26900.0\nStarting run = 27000.0\nStarting run = 27100.0\nStarting run = 27200.0\nStarting run = 27300.0\nStarting run = 27400.0\nStarting run = 27500.0\nStarting run = 27600.0\nStarting run = 27700.0\nStarting run = 27800.0\nStarting run = 27900.0\nStarting run = 28000.0\nStarting run = 28100.0\nStarting run = 28200.0\nStarting run = 28300.0\nStarting run = 28400.0\nStarting run = 28500.0\nStarting run = 28600.0\nStarting run = 28700.0\nStarting run = 28800.0\nStarting run = 28900.0\nStarting run = 29000.0\nStarting run = 29100.0\nStarting run = 29200.0\nStarting run = 29300.0\nStarting run = 29400.0\nStarting run = 29500.0\nStarting run = 29600.0\nStarting run = 29700.0\nStarting run = 29800.0\nStarting run = 29900.0\nStarting run = 30000.0\nStarting run = 30100.0\nStarting run = 30200.0\nStarting run = 30300.0\nStarting run = 30400.0\nStarting run = 30500.0\nStarting run = 30600.0\nStarting run = 30700.0\nStarting run = 30800.0\nStarting run = 30900.0\nStarting run = 31000.0\nStarting run = 31100.0\nStarting run = 31200.0\nStarting run = 31300.0\nStarting run = 31400.0\nStarting run = 31500.0\nStarting run = 31600.0\nStarting run = 31700.0\nStarting run = 31800.0\nStarting run = 31900.0\nStarting run = 32000.0\nStarting run = 32100.0\nStarting run = 32200.0\nStarting run = 32300.0\nStarting run = 32400.0\nStarting run = 32500.0\nStarting run = 32600.0\nStarting run = 32700.0\nStarting run = 32800.0\nStarting run = 32900.0\nStarting run = 33000.0\nStarting run = 33100.0\nStarting run = 33200.0\nStarting run = 33300.0\nStarting run = 33400.0\nStarting run = 33500.0\nStarting run = 33600.0\nStarting run = 33700.0\nStarting run = 33800.0\nStarting run = 33900.0\nStarting run = 34000.0\nStarting run = 34100.0\nStarting run = 34200.0\nStarting run = 34300.0\nStarting run = 34400.0\nStarting run = 34500.0\nStarting run = 34600.0\nStarting run = 34700.0\nStarting run = 34800.0\nStarting run = 34900.0\nStarting run = 35000.0\nStarting run = 35100.0\nStarting run = 35200.0\nStarting run = 35300.0\nStarting run = 35400.0\nStarting run = 35500.0\nStarting run = 35600.0\nStarting run = 35700.0\nStarting run = 35800.0\nStarting run = 35900.0\nStarting run = 36000.0\nStarting run = 36100.0\nStarting run = 36200.0\nStarting run = 36300.0\nStarting run = 36400.0\nStarting run = 36500.0\nStarting run = 36600.0\nStarting run = 36700.0\nStarting run = 36800.0\nStarting run = 36900.0\nStarting run = 37000.0\nStarting run = 37100.0\nStarting run = 37200.0\nStarting run = 37300.0\nStarting run = 37400.0\nStarting run = 37500.0\nStarting run = 37600.0\nStarting run = 37700.0\nStarting run = 37800.0\nStarting run = 37900.0\nStarting run = 38000.0\nStarting run = 38100.0\nStarting run = 38200.0\nStarting run = 38300.0\nStarting run = 38400.0\nStarting run = 38500.0\nStarting run = 38600.0\nStarting run = 38700.0\nStarting run = 38800.0\nStarting run = 38900.0\nStarting run = 39000.0\nStarting run = 39100.0\nStarting run = 39200.0\nStarting run = 39300.0\nStarting run = 39400.0\nStarting run = 39500.0\nStarting run = 39600.0\nStarting run = 39700.0\nStarting run = 39800.0\nStarting run = 39900.0\nStarting run = 40000.0\nStarting run = 40100.0\nStarting run = 40200.0\nStarting run = 40300.0\nStarting run = 40400.0\nStarting run = 40500.0\nStarting run = 40600.0\nStarting run = 40700.0\nStarting run = 40800.0\nStarting run = 40900.0\nStarting run = 41000.0\nStarting run = 41100.0\nStarting run = 41200.0\nStarting run = 41300.0\nStarting run = 41400.0\nStarting run = 41500.0\nStarting run = 41600.0\nStarting run = 41700.0\nStarting run = 41800.0\nStarting run = 41900.0\nStarting run = 42000.0\nStarting run = 42100.0\nStarting run = 42200.0\nStarting run = 42300.0\nStarting run = 42400.0\nStarting run = 42500.0\nStarting run = 42600.0\nStarting run = 42700.0\nStarting run = 42800.0\nStarting run = 42900.0\nStarting run = 43000.0\nStarting run = 43100.0\nStarting run = 43200.0\nStarting run = 43300.0\nStarting run = 43400.0\nStarting run = 43500.0\nStarting run = 43600.0\nStarting run = 43700.0\nStarting run = 43800.0\nStarting run = 43900.0\nStarting run = 44000.0\nStarting run = 44100.0\nStarting run = 44200.0\nStarting run = 44300.0\nStarting run = 44400.0\nStarting run = 44500.0\nStarting run = 44600.0\nStarting run = 44700.0\nStarting run = 44800.0\nStarting run = 44900.0\nStarting run = 45000.0\nStarting run = 45100.0\nStarting run = 45200.0\nStarting run = 45300.0\nStarting run = 45400.0\nStarting run = 45500.0\nStarting run = 45600.0\nStarting run = 45700.0\nStarting run = 45800.0\nStarting run = 45900.0\nStarting run = 46000.0\nStarting run = 46100.0\nStarting run = 46200.0\nStarting run = 46300.0\nStarting run = 46400.0\nStarting run = 46500.0\nStarting run = 46600.0\nStarting run = 46700.0\nStarting run = 46800.0\nStarting run = 46900.0\nStarting run = 47000.0\nStarting run = 47100.0\nStarting run = 47200.0\nStarting run = 47300.0\nStarting run = 47400.0\nStarting run = 47500.0\nStarting run = 47600.0\nStarting run = 47700.0\nStarting run = 47800.0\nStarting run = 47900.0\nStarting run = 48000.0\nStarting run = 48100.0\nStarting run = 48200.0\nStarting run = 48300.0\nStarting run = 48400.0\nStarting run = 48500.0\nStarting run = 48600.0\nStarting run = 48700.0\nStarting run = 48800.0\nStarting run = 48900.0\nStarting run = 49000.0\nStarting run = 49100.0\nStarting run = 49200.0\nStarting run = 49300.0\nStarting run = 49400.0\nStarting run = 49500.0\nStarting run = 49600.0\nStarting run = 49700.0\nStarting run = 49800.0\nStarting run = 49900.0\nStarting run = 50000.0\nStarting run = 50100.0\nStarting run = 50200.0\nStarting run = 50300.0\nStarting run = 50400.0\nStarting run = 50500.0\nStarting run = 50600.0\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(2700, 16200)"
     },
     "metadata": {},
     "execution_count": 314
    }
   ],
   "source": [
    "# Putting the matrix back together\n",
    "result = np.array([[0 for x in range(16200)] for x in range(2700)], dtype = 'float32')\n",
    "encDec = encDec.reshape(reshapeData.shape[0], reshapeData.shape[1], reshapeData.shape[2])\n",
    "\n",
    "for i in range(0, 2700, 12):   # cells\n",
    "    for j in range(0, 16200, 72):   # genes\n",
    "        x = ( 225*(i/12) ) + ( (j/72)+1 )\n",
    "        if x%100 == 0:\n",
    "            print(\"Starting run =\", x)\n",
    "        sect = encDec[int(x-1),:,:]\n",
    "        result[i:(i+12) , j:(j+72)] = sect\n",
    "    \n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "         0         1         2         3         4         5         6      \\\n0     0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536   \n1     0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536   \n2     0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536   \n3     0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536   \n4     0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536   \n...        ...       ...       ...       ...       ...       ...       ...   \n2695  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536   \n2696  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536   \n2697  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536   \n2698  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536   \n2699  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536   \n\n         7         8         9      ...     16190     16191     16192  \\\n0     0.024536  0.024536  0.024536  ...  0.024536  0.024536  0.024536   \n1     0.024536  0.024536  0.024536  ...  0.024536  0.024536  0.024536   \n2     0.024536  0.024536  0.024536  ...  0.024536  0.024536  0.024536   \n3     0.024536  0.024536  0.024536  ...  0.024536  0.024536  0.024536   \n4     0.024536  0.024536  0.024536  ...  0.024536  0.024536  0.024536   \n...        ...       ...       ...  ...       ...       ...       ...   \n2695  0.024536  0.024536  0.024536  ...  0.024536  0.024536  0.024536   \n2696  0.024536  0.024536  0.024536  ...  0.024536  0.024536  0.024536   \n2697  0.024536  0.024536  0.024536  ...  0.024536  0.024536  0.024536   \n2698  0.024536  0.024536  0.024536  ...  0.024536  0.024536  0.024536   \n2699  0.024536  0.024536  0.024536  ...  0.024536  0.024536  0.024536   \n\n         16193     16194     16195     16196     16197     16198     16199  \n0     0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  \n1     0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  \n2     0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  \n3     0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  \n4     0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  \n...        ...       ...       ...       ...       ...       ...       ...  \n2695  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  \n2696  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  \n2697  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  \n2698  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  \n2699  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  \n\n[2700 rows x 16200 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>16190</th>\n      <th>16191</th>\n      <th>16192</th>\n      <th>16193</th>\n      <th>16194</th>\n      <th>16195</th>\n      <th>16196</th>\n      <th>16197</th>\n      <th>16198</th>\n      <th>16199</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>...</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>...</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>...</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>...</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>...</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2695</th>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>...</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n    </tr>\n    <tr>\n      <th>2696</th>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>...</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n    </tr>\n    <tr>\n      <th>2697</th>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>...</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n    </tr>\n    <tr>\n      <th>2698</th>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>...</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n    </tr>\n    <tr>\n      <th>2699</th>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>...</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n    </tr>\n  </tbody>\n</table>\n<p>2700 rows × 16200 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 332
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0"
     },
     "metadata": {},
     "execution_count": 332
    }
   ],
   "source": [
    "result_df = pd.DataFrame(result)\n",
    "result_df\n",
    "\n",
    "sum(result_df.sum(axis = 1) != 397.483429)\n",
    "# Outputed matrix filled with all 0.024536\n",
    "# something went wrong..."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596188517751",
   "display_name": "Python 3.8.3 64-bit ('tf': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}