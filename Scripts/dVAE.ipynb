{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "import scanpy as sc\n",
    "from math import log\n",
    "from statistics import median\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "import keras.losses\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read_10x_mtx(\n",
    "    '/Volumes/Samsung_T5/ResearchData/scanpyTutorial/data/filtered_gene_bc_matrices/hg19',  # the directory with the `.mtx` file\n",
    "    var_names='gene_symbols',                      # use gene symbols for the variable names (variables-axis index)\n",
    "    cache=True)\n",
    "\n",
    "adata.var_names_make_unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Working on 2700 cells and 32738 genes\n"
    }
   ],
   "source": [
    "data = pd.DataFrame.sparse.from_spmatrix(adata.X)\n",
    "print('Working on {} cells and {} genes'.format(*data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(2700, 16634)"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# Filter out genes that are not expressed in any cells\n",
    "geneSum = data.sum(axis=0)\n",
    "x = geneSum.index[geneSum == 0].tolist()\n",
    "data = data.drop(x, axis = 1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing data (using method from Rao, et al.)\n",
    "cellSum  = data.sum(axis=1)\n",
    "median_j = median(cellSum)\n",
    "npData = np.asarray(data)\n",
    "for j in range(2700):\n",
    "    cellSum_j = cellSum[j]\n",
    "    for i in range(16634):\n",
    "        npData[j,i] = log( ( (npData[j,i])/(cellSum_j) * median_j ) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "      0      1      2      3      4      5      6      7      8      9      \\\n0       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n1       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n2       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n3       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n4       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n...     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n2695    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n2696    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n2697    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n2698    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n2699    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n\n      ...  16624     16625  16626  16627     16628  16629     16630  16631  \\\n0     ...    0.0  1.532536    0.0    0.0  0.000000    0.0  0.000000    0.0   \n1     ...    0.0  1.522734    0.0    0.0  0.370248    0.0  0.000000    0.0   \n2     ...    0.0  1.332558    0.0    0.0  0.000000    0.0  0.000000    0.0   \n3     ...    0.0  0.980213    0.0    0.0  0.000000    0.0  0.000000    0.0   \n4     ...    0.0  1.175435    0.0    0.0  0.000000    0.0  0.000000    0.0   \n...   ...    ...       ...    ...    ...       ...    ...       ...    ...   \n2695  ...    0.0  0.491513    0.0    0.0  0.000000    0.0  0.000000    0.0   \n2696  ...    0.0  1.266796    0.0    0.0  0.000000    0.0  0.000000    0.0   \n2697  ...    0.0  1.827533    0.0    0.0  0.000000    0.0  0.000000    0.0   \n2698  ...    0.0  1.145975    0.0    0.0  0.000000    0.0  1.145975    0.0   \n2699  ...    0.0  1.463349    0.0    0.0  0.000000    0.0  0.000000    0.0   \n\n      16632  16633  \n0       0.0    0.0  \n1       0.0    0.0  \n2       0.0    0.0  \n3       0.0    0.0  \n4       0.0    0.0  \n...     ...    ...  \n2695    0.0    0.0  \n2696    0.0    0.0  \n2697    0.0    0.0  \n2698    0.0    0.0  \n2699    0.0    0.0  \n\n[2700 rows x 16634 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>16624</th>\n      <th>16625</th>\n      <th>16626</th>\n      <th>16627</th>\n      <th>16628</th>\n      <th>16629</th>\n      <th>16630</th>\n      <th>16631</th>\n      <th>16632</th>\n      <th>16633</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.532536</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.522734</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.370248</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.332558</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.980213</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.175435</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2695</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.491513</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2696</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.266796</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2697</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.827533</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2698</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.145975</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>1.145975</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2699</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.463349</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2700 rows × 16634 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "dataNorm = pd.DataFrame(npData)\n",
    "dataNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Autoencoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Info from TF tutorial below, but model not actually implemented)\n",
    "- Use 2 small ConvNets for the encoder and decoder networks (inference/recognition and generative models)\n",
    "- *x* = observation and *z* = latent variable\n",
    "- Encoder network defines the approximate posterior distribution *q(z|x)*\n",
    "    - Use 2 convolutional layers followed by a fully-connected layer\n",
    "- Decoder network defines the approximate conditional distribution *p(x|z)*\n",
    "    - Use a fully-connected layer followed by three convolution transpose layers\n",
    "- Reparameterization Trick...\n",
    "    - Backpropagation cannot flow through random node\n",
    "    - Approximate *z* using the decoder parameters and another parameter $\\epsilon$ as follows: \n",
    "        - $z = \\mu + \\sigma \\odot \\epsilon$\n",
    "        - where $\\mu$ and $\\sigma$ represent the mean and standard deviation of a Gaussian distribution respectively\n",
    "    - Enables model to backpropagate gradients in the encoder through $\\mu$ and $\\sigma$ while maintaining stochasticity through $\\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational AutoEncoder (keras tutorial, but 1 Dimensional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going to try dividing matrix into small \"images\" \n",
    "(2700 x 16634) --> 225 (12 x 72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(2700, 16200)"
     },
     "metadata": {},
     "execution_count": 195
    }
   ],
   "source": [
    "VMR = dataNorm.std() / dataNorm.mean() \n",
    "lowestVMR = VMR.sort_values(ascending=False)[(225*72):]\n",
    "fullData = np.asarray(dataNorm.drop(lowestVMR.index, axis= 1))\n",
    "fullData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Starting i = 1.0\nStarting i = 2.0\nStarting i = 3.0\nStarting i = 4.0\nStarting i = 5.0\nStarting i = 6.0\nStarting i = 7.0\nStarting i = 8.0\nStarting i = 9.0\nStarting i = 10.0\nStarting i = 11.0\nStarting i = 12.0\nStarting i = 13.0\nStarting i = 14.0\nStarting i = 15.0\nStarting i = 16.0\nStarting i = 17.0\nStarting i = 18.0\nStarting i = 19.0\nStarting i = 20.0\nStarting i = 21.0\nStarting i = 22.0\nStarting i = 23.0\nStarting i = 24.0\nStarting i = 25.0\nStarting i = 26.0\nStarting i = 27.0\nStarting i = 28.0\nStarting i = 29.0\nStarting i = 30.0\nStarting i = 31.0\nStarting i = 32.0\nStarting i = 33.0\nStarting i = 34.0\nStarting i = 35.0\nStarting i = 36.0\nStarting i = 37.0\nStarting i = 38.0\nStarting i = 39.0\nStarting i = 40.0\nStarting i = 41.0\nStarting i = 42.0\nStarting i = 43.0\nStarting i = 44.0\nStarting i = 45.0\nStarting i = 46.0\nStarting i = 47.0\nStarting i = 48.0\nStarting i = 49.0\nStarting i = 50.0\nStarting i = 51.0\nStarting i = 52.0\nStarting i = 53.0\nStarting i = 54.0\nStarting i = 55.0\nStarting i = 56.0\nStarting i = 57.0\nStarting i = 58.0\nStarting i = 59.0\nStarting i = 60.0\nStarting i = 61.0\nStarting i = 62.0\nStarting i = 63.0\nStarting i = 64.0\nStarting i = 65.0\nStarting i = 66.0\nStarting i = 67.0\nStarting i = 68.0\nStarting i = 69.0\nStarting i = 70.0\nStarting i = 71.0\nStarting i = 72.0\nStarting i = 73.0\nStarting i = 74.0\nStarting i = 75.0\nStarting i = 76.0\nStarting i = 77.0\nStarting i = 78.0\nStarting i = 79.0\nStarting i = 80.0\nStarting i = 81.0\nStarting i = 82.0\nStarting i = 83.0\nStarting i = 84.0\nStarting i = 85.0\nStarting i = 86.0\nStarting i = 87.0\nStarting i = 88.0\nStarting i = 89.0\nStarting i = 90.0\nStarting i = 91.0\nStarting i = 92.0\nStarting i = 93.0\nStarting i = 94.0\nStarting i = 95.0\nStarting i = 96.0\nStarting i = 97.0\nStarting i = 98.0\nStarting i = 99.0\nStarting i = 100.0\nStarting i = 101.0\nStarting i = 102.0\nStarting i = 103.0\nStarting i = 104.0\nStarting i = 105.0\nStarting i = 106.0\nStarting i = 107.0\nStarting i = 108.0\nStarting i = 109.0\nStarting i = 110.0\nStarting i = 111.0\nStarting i = 112.0\nStarting i = 113.0\nStarting i = 114.0\nStarting i = 115.0\nStarting i = 116.0\nStarting i = 117.0\nStarting i = 118.0\nStarting i = 119.0\nStarting i = 120.0\nStarting i = 121.0\nStarting i = 122.0\nStarting i = 123.0\nStarting i = 124.0\nStarting i = 125.0\nStarting i = 126.0\nStarting i = 127.0\nStarting i = 128.0\nStarting i = 129.0\nStarting i = 130.0\nStarting i = 131.0\nStarting i = 132.0\nStarting i = 133.0\nStarting i = 134.0\nStarting i = 135.0\nStarting i = 136.0\nStarting i = 137.0\nStarting i = 138.0\nStarting i = 139.0\nStarting i = 140.0\nStarting i = 141.0\nStarting i = 142.0\nStarting i = 143.0\nStarting i = 144.0\nStarting i = 145.0\nStarting i = 146.0\nStarting i = 147.0\nStarting i = 148.0\nStarting i = 149.0\nStarting i = 150.0\nStarting i = 151.0\nStarting i = 152.0\nStarting i = 153.0\nStarting i = 154.0\nStarting i = 155.0\nStarting i = 156.0\nStarting i = 157.0\nStarting i = 158.0\nStarting i = 159.0\nStarting i = 160.0\nStarting i = 161.0\nStarting i = 162.0\nStarting i = 163.0\nStarting i = 164.0\nStarting i = 165.0\nStarting i = 166.0\nStarting i = 167.0\nStarting i = 168.0\nStarting i = 169.0\nStarting i = 170.0\nStarting i = 171.0\nStarting i = 172.0\nStarting i = 173.0\nStarting i = 174.0\nStarting i = 175.0\nStarting i = 176.0\nStarting i = 177.0\nStarting i = 178.0\nStarting i = 179.0\nStarting i = 180.0\nStarting i = 181.0\nStarting i = 182.0\nStarting i = 183.0\nStarting i = 184.0\nStarting i = 185.0\nStarting i = 186.0\nStarting i = 187.0\nStarting i = 188.0\nStarting i = 189.0\nStarting i = 190.0\nStarting i = 191.0\nStarting i = 192.0\nStarting i = 193.0\nStarting i = 194.0\nStarting i = 195.0\nStarting i = 196.0\nStarting i = 197.0\nStarting i = 198.0\nStarting i = 199.0\nStarting i = 200.0\nStarting i = 201.0\nStarting i = 202.0\nStarting i = 203.0\nStarting i = 204.0\nStarting i = 205.0\nStarting i = 206.0\nStarting i = 207.0\nStarting i = 208.0\nStarting i = 209.0\nStarting i = 210.0\nStarting i = 211.0\nStarting i = 212.0\nStarting i = 213.0\nStarting i = 214.0\nStarting i = 215.0\nStarting i = 216.0\nStarting i = 217.0\nStarting i = 218.0\nStarting i = 219.0\nStarting i = 220.0\nStarting i = 221.0\nStarting i = 222.0\nStarting i = 223.0\nStarting i = 224.0\nStarting i = 225.0\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(50625, 12, 72, 1)"
     },
     "metadata": {},
     "execution_count": 241
    }
   ],
   "source": [
    "# \"Chunking\" the matrix \n",
    "reshapeData = np.array([[[0 for x in range(72)] for x in range(12)] for x in range(50625)], dtype = 'float32')\n",
    "for i in range(0, 2700, 12):   # i refers to cells (12)\n",
    "    print(\"Starting i =\", (i/12)+1)\n",
    "    for j in range(0, 16200, 72):   # j refers to genes (72)\n",
    "        x = ( 225*(i/12) ) + ( (j/72)+1 )\n",
    "        smallMat = fullData[ i:(i+12) , j:(j+72) ]\n",
    "        reshapeData[int(x-1),:,:] = smallMat\n",
    "\n",
    "reshapeData = reshapeData.reshape((reshapeData.shape[0], reshapeData.shape[1], reshapeData.shape[2], 1))\n",
    "reshapeData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sampling layer\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"encoder\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_16 (InputLayer)           [(None, 12, 72, 1)]  0                                            \n__________________________________________________________________________________________________\nconv2d_14 (Conv2D)              (None, 6, 36, 32)    320         input_16[0][0]                   \n__________________________________________________________________________________________________\nconv2d_15 (Conv2D)              (None, 3, 18, 64)    18496       conv2d_14[0][0]                  \n__________________________________________________________________________________________________\nflatten_8 (Flatten)             (None, 3456)         0           conv2d_15[0][0]                  \n__________________________________________________________________________________________________\ndense_15 (Dense)                (None, 16)           55312       flatten_8[0][0]                  \n__________________________________________________________________________________________________\nz_mean (Dense)                  (None, 32)           544         dense_15[0][0]                   \n__________________________________________________________________________________________________\nz_log_var (Dense)               (None, 32)           544         dense_15[0][0]                   \n__________________________________________________________________________________________________\nsampling_8 (Sampling)           (None, 32)           0           z_mean[0][0]                     \n                                                                 z_log_var[0][0]                  \n==================================================================================================\nTotal params: 75,216\nTrainable params: 75,216\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
    }
   ],
   "source": [
    "# Build the encoder\n",
    "latent_dim = 32\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(12, 72, 1))\n",
    "x = layers.Conv2D(filters= 32, kernel_size= 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "x = layers.Conv2D(filters= 64, kernel_size= 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"decoder\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_18 (InputLayer)        [(None, 32)]              0         \n_________________________________________________________________\ndense_17 (Dense)             (None, 3456)              114048    \n_________________________________________________________________\nreshape_8 (Reshape)          (None, 3, 18, 64)         0         \n_________________________________________________________________\nconv2d_transpose_21 (Conv2DT (None, 6, 36, 64)         36928     \n_________________________________________________________________\nconv2d_transpose_22 (Conv2DT (None, 12, 72, 32)        18464     \n_________________________________________________________________\nconv2d_transpose_23 (Conv2DT (None, 12, 72, 1)         289       \n=================================================================\nTotal params: 169,729\nTrainable params: 169,729\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "# Build the decoder\n",
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(3 * 18 * 64, activation=\"relu\")(latent_inputs)\n",
    "x = layers.Reshape((3, 18, 64))(x)\n",
    "x = layers.Conv2DTranspose(filters= 64, kernel_size= 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Conv2DTranspose(filters= 32, kernel_size= 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the VAE as a Model with a custom train_step\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def train_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = encoder(data)\n",
    "            reconstruction = decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                keras.losses.mean_squared_error(data, reconstruction)\n",
    "            )\n",
    "            reconstruction_loss *= 12 * 72\n",
    "            kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "            kl_loss = tf.reduce_mean(kl_loss)\n",
    "            kl_loss *= -0.5\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"reconstruction_loss\": reconstruction_loss,\n",
    "            \"kl_loss\": kl_loss,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/50\n792/792 [==============================] - 32s 41ms/step - loss: 18.0740 - reconstruction_loss: 17.6169 - kl_loss: 0.4571\nEpoch 2/50\n792/792 [==============================] - 34s 43ms/step - loss: 17.6948 - reconstruction_loss: 17.3001 - kl_loss: 0.3946\nEpoch 3/50\n792/792 [==============================] - 36s 45ms/step - loss: 17.5447 - reconstruction_loss: 17.1862 - kl_loss: 0.3585\nEpoch 4/50\n792/792 [==============================] - 40s 51ms/step - loss: 17.4276 - reconstruction_loss: 17.1040 - kl_loss: 0.3236\nEpoch 5/50\n792/792 [==============================] - 36s 46ms/step - loss: 17.3525 - reconstruction_loss: 17.0408 - kl_loss: 0.3117\nEpoch 6/50\n792/792 [==============================] - 36s 45ms/step - loss: 17.3350 - reconstruction_loss: 17.0348 - kl_loss: 0.3002\nEpoch 7/50\n792/792 [==============================] - 37s 47ms/step - loss: 17.3511 - reconstruction_loss: 17.0646 - kl_loss: 0.2865\nEpoch 8/50\n792/792 [==============================] - 34s 43ms/step - loss: 17.3766 - reconstruction_loss: 17.1070 - kl_loss: 0.2696\nEpoch 9/50\n792/792 [==============================] - 36s 45ms/step - loss: 17.2904 - reconstruction_loss: 17.0410 - kl_loss: 0.2494\nEpoch 10/50\n792/792 [==============================] - 36s 46ms/step - loss: 17.2630 - reconstruction_loss: 17.0376 - kl_loss: 0.2254\nEpoch 11/50\n792/792 [==============================] - 37s 47ms/step - loss: 17.2469 - reconstruction_loss: 17.0499 - kl_loss: 0.1971\nEpoch 12/50\n792/792 [==============================] - 36s 45ms/step - loss: 17.2295 - reconstruction_loss: 17.0652 - kl_loss: 0.1643\nEpoch 13/50\n792/792 [==============================] - 33s 42ms/step - loss: 17.2013 - reconstruction_loss: 17.0715 - kl_loss: 0.1298\nEpoch 14/50\n792/792 [==============================] - 36s 45ms/step - loss: 17.1901 - reconstruction_loss: 17.0920 - kl_loss: 0.0981\nEpoch 15/50\n792/792 [==============================] - 36s 46ms/step - loss: 17.1452 - reconstruction_loss: 17.0632 - kl_loss: 0.0819\nEpoch 16/50\n792/792 [==============================] - 37s 46ms/step - loss: 17.1347 - reconstruction_loss: 17.0553 - kl_loss: 0.0793\nEpoch 17/50\n792/792 [==============================] - 37s 46ms/step - loss: 17.1084 - reconstruction_loss: 17.0344 - kl_loss: 0.0740\nEpoch 18/50\n792/792 [==============================] - 36s 45ms/step - loss: 17.1076 - reconstruction_loss: 17.0416 - kl_loss: 0.0660\nEpoch 19/50\n792/792 [==============================] - 37s 47ms/step - loss: 17.1282 - reconstruction_loss: 17.0724 - kl_loss: 0.0557\nEpoch 20/50\n792/792 [==============================] - 37s 46ms/step - loss: 17.0979 - reconstruction_loss: 17.0543 - kl_loss: 0.0436\nEpoch 21/50\n792/792 [==============================] - 36s 46ms/step - loss: 17.1032 - reconstruction_loss: 17.0673 - kl_loss: 0.0359\nEpoch 22/50\n792/792 [==============================] - 37s 47ms/step - loss: 17.0987 - reconstruction_loss: 17.0683 - kl_loss: 0.0304\nEpoch 23/50\n792/792 [==============================] - 36s 46ms/step - loss: 17.0646 - reconstruction_loss: 17.0366 - kl_loss: 0.0280\nEpoch 24/50\n792/792 [==============================] - 37s 47ms/step - loss: 17.0523 - reconstruction_loss: 17.0241 - kl_loss: 0.0282\nEpoch 25/50\n792/792 [==============================] - 37s 47ms/step - loss: 17.0751 - reconstruction_loss: 17.0477 - kl_loss: 0.0274\nEpoch 26/50\n792/792 [==============================] - 36s 46ms/step - loss: 17.0521 - reconstruction_loss: 17.0258 - kl_loss: 0.0263\nEpoch 27/50\n792/792 [==============================] - 38s 47ms/step - loss: 17.0749 - reconstruction_loss: 17.0499 - kl_loss: 0.0250\nEpoch 28/50\n792/792 [==============================] - 37s 46ms/step - loss: 17.0747 - reconstruction_loss: 17.0515 - kl_loss: 0.0233\nEpoch 29/50\n792/792 [==============================] - 37s 46ms/step - loss: 17.0418 - reconstruction_loss: 17.0205 - kl_loss: 0.0214\nEpoch 30/50\n792/792 [==============================] - 38s 47ms/step - loss: 17.0452 - reconstruction_loss: 17.0257 - kl_loss: 0.0195\nEpoch 31/50\n792/792 [==============================] - 37s 46ms/step - loss: 17.0733 - reconstruction_loss: 17.0561 - kl_loss: 0.0172\nEpoch 32/50\n792/792 [==============================] - 38s 48ms/step - loss: 17.0613 - reconstruction_loss: 17.0467 - kl_loss: 0.0146\nEpoch 33/50\n792/792 [==============================] - 38s 47ms/step - loss: 17.0360 - reconstruction_loss: 17.0218 - kl_loss: 0.0142\nEpoch 34/50\n792/792 [==============================] - 36s 46ms/step - loss: 17.0593 - reconstruction_loss: 17.0476 - kl_loss: 0.0117\nEpoch 35/50\n792/792 [==============================] - 37s 47ms/step - loss: 17.0665 - reconstruction_loss: 17.0568 - kl_loss: 0.0097\nEpoch 36/50\n792/792 [==============================] - 36s 45ms/step - loss: 17.0552 - reconstruction_loss: 17.0478 - kl_loss: 0.0074\nEpoch 37/50\n792/792 [==============================] - 37s 46ms/step - loss: 17.0322 - reconstruction_loss: 17.0264 - kl_loss: 0.0058\nEpoch 38/50\n792/792 [==============================] - 36s 46ms/step - loss: 17.0767 - reconstruction_loss: 17.0724 - kl_loss: 0.0043\nEpoch 39/50\n792/792 [==============================] - 36s 45ms/step - loss: 17.0557 - reconstruction_loss: 17.0523 - kl_loss: 0.0034\nEpoch 40/50\n792/792 [==============================] - 36s 46ms/step - loss: 17.0359 - reconstruction_loss: 17.0332 - kl_loss: 0.0027\nEpoch 41/50\n792/792 [==============================] - 36s 45ms/step - loss: 17.0418 - reconstruction_loss: 17.0398 - kl_loss: 0.0021\nEpoch 42/50\n792/792 [==============================] - 36s 45ms/step - loss: 17.0249 - reconstruction_loss: 17.0237 - kl_loss: 0.0011\nEpoch 43/50\n792/792 [==============================] - 35s 45ms/step - loss: 17.0653 - reconstruction_loss: 17.0645 - kl_loss: 7.3957e-04\nEpoch 44/50\n792/792 [==============================] - 35s 45ms/step - loss: 17.0401 - reconstruction_loss: 17.0398 - kl_loss: 2.5749e-04\nEpoch 45/50\n792/792 [==============================] - 35s 44ms/step - loss: 17.0298 - reconstruction_loss: 17.0297 - kl_loss: 4.2766e-05\nEpoch 46/50\n792/792 [==============================] - 35s 44ms/step - loss: 17.0346 - reconstruction_loss: 17.0346 - kl_loss: 3.4714e-06\nEpoch 47/50\n792/792 [==============================] - 35s 44ms/step - loss: 17.0290 - reconstruction_loss: 17.0290 - kl_loss: 9.4074e-08\nEpoch 48/50\n792/792 [==============================] - 35s 44ms/step - loss: 17.0684 - reconstruction_loss: 17.0684 - kl_loss: 4.5803e-10\nEpoch 49/50\n792/792 [==============================] - 36s 45ms/step - loss: 17.0336 - reconstruction_loss: 17.0336 - kl_loss: 0.0000e+00\nEpoch 50/50\n792/792 [==============================] - 35s 44ms/step - loss: 17.0312 - reconstruction_loss: 17.0312 - kl_loss: 0.0000e+00\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7fbaddac2130>"
     },
     "metadata": {},
     "execution_count": 287
    }
   ],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())\n",
    "vae.fit(reshapeData, epochs=50, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encDec = decoder.predict(encoder.predict(reshapeData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[[[0.02453601],\n         [0.02453601],\n         [0.02453601],\n         ...,\n         [0.02453601],\n         [0.02453601],\n         [0.02453601]],\n\n        [[0.02453601],\n         [0.02453601],\n         [0.02453601],\n         ...,\n         [0.02453601],\n         [0.02453601],\n         [0.02453601]],\n\n        [[0.02453601],\n         [0.02453601],\n         [0.02453601],\n         ...,\n         [0.02453601],\n         [0.02453601],\n         [0.02453601]],\n\n        ...,\n\n        [[0.02453601],\n         [0.02453601],\n         [0.02453601],\n         ...,\n         [0.02453601],\n         [0.02453601],\n         [0.02453601]],\n\n        [[0.02453601],\n         [0.02453601],\n         [0.02453601],\n         ...,\n         [0.02453601],\n         [0.02453601],\n         [0.02453601]],\n\n        [[0.02453601],\n         [0.02453601],\n         [0.02453601],\n         ...,\n         [0.02453601],\n         [0.02453601],\n         [0.02453601]]],\n\n\n       [[[0.02453601],\n         [0.02453601],\n         [0.02453601],\n         ...,\n         [0.02453601],\n         [0.02453601],\n         [0.02453601]],\n\n        [[0.02453601],\n         [0.02453601],\n         [0.02453601],\n         ...,\n         [0.02453601],\n         [0.02453601],\n         [0.02453601]],\n\n        [[0.02453601],\n         [0.02453601],\n         [0.02453601],\n         ...,\n         [0.02453601],\n         [0.02453601],\n         [0.02453601]],\n\n        ...,\n\n        [[0.02453601],\n         [0.02453601],\n         [0.02453601],\n         ...,\n         [0.02453601],\n         [0.02453601],\n         [0.02453601]],\n\n        [[0.02453601],\n         [0.02453601],\n         [0.02453601],\n         ...,\n         [0.02453601],\n         [0.02453601],\n         [0.02453601]],\n\n        [[0.02453601],\n         [0.02453601],\n         [0.02453601],\n         ...,\n         [0.02453601],\n         [0.02453601],\n         [0.02453601]]],\n\n\n       [[[0.02453601],\n         [0.02453601],\n         [0.02453601],\n         ...,\n         [0.02453601],\n         [0.02453601],\n         [0.02453601]],\n\n        [[0.02453601],\n         [0.02453601],\n         [0.02453601],\n         ...,\n         [0.02453601],\n         [0.02453601],\n         [0.02453601]],\n\n        [[0.02453601],\n         [0.02453601],\n         [0.02453601],\n         ...,\n         [0.02453601],\n         [0.02453601],\n         [0.02453601]],\n\n        ...,\n\n        [[0.02453601],\n         [0.02453601],\n         [0.02453601],\n         ...,\n         [0.02453601],\n         [0.02453601],\n         [0.02453601]],\n\n        [[0.02453601],\n         [0.02453601],\n         [0.02453601],\n         ...,\n         [0.02453601],\n         [0.02453601],\n         [0.02453601]],\n\n        [[0.02453601],\n         [0.02453601],\n         [0.02453601],\n         ...,\n         [0.02453601],\n         [0.02453601],\n         [0.02453601]]],\n\n\n       ...,\n\n\n       [[[0.02453601],\n         [0.02453601],\n         [0.02453601],\n         ...,\n         [0.02453601],\n         [0.02453601],\n         [0.02453601]],\n\n        [[0.02453601],\n         [0.02453601],\n         [0.02453601],\n         ...,\n         [0.02453601],\n         [0.02453601],\n         [0.02453601]],\n\n        [[0.02453601],\n         [0.02453601],\n         [0.02453601],\n         ...,\n         [0.02453601],\n         [0.02453601],\n         [0.02453601]],\n\n        ...,\n\n        [[0.02453601],\n         [0.02453601],\n         [0.02453601],\n         ...,\n         [0.02453601],\n         [0.02453601],\n         [0.02453601]],\n\n        [[0.02453601],\n         [0.02453601],\n         [0.02453601],\n         ...,\n         [0.02453601],\n         [0.02453601],\n         [0.02453601]],\n\n        [[0.02453601],\n         [0.02453601],\n         [0.02453601],\n         ...,\n         [0.02453601],\n         [0.02453601],\n         [0.02453601]]],\n\n\n       [[[0.02453601],\n         [0.02453601],\n         [0.02453601],\n         ...,\n         [0.02453601],\n         [0.02453601],\n         [0.02453601]],\n\n        [[0.02453601],\n         [0.02453601],\n         [0.02453601],\n         ...,\n         [0.02453601],\n         [0.02453601],\n         [0.02453601]],\n\n        [[0.02453601],\n         [0.02453601],\n         [0.02453601],\n         ...,\n         [0.02453601],\n         [0.02453601],\n         [0.02453601]],\n\n        ...,\n\n        [[0.02453601],\n         [0.02453601],\n         [0.02453601],\n         ...,\n         [0.02453601],\n         [0.02453601],\n         [0.02453601]],\n\n        [[0.02453601],\n         [0.02453601],\n         [0.02453601],\n         ...,\n         [0.02453601],\n         [0.02453601],\n         [0.02453601]],\n\n        [[0.02453601],\n         [0.02453601],\n         [0.02453601],\n         ...,\n         [0.02453601],\n         [0.02453601],\n         [0.02453601]]],\n\n\n       [[[0.02453601],\n         [0.02453601],\n         [0.02453601],\n         ...,\n         [0.02453601],\n         [0.02453601],\n         [0.02453601]],\n\n        [[0.02453601],\n         [0.02453601],\n         [0.02453601],\n         ...,\n         [0.02453601],\n         [0.02453601],\n         [0.02453601]],\n\n        [[0.02453601],\n         [0.02453601],\n         [0.02453601],\n         ...,\n         [0.02453601],\n         [0.02453601],\n         [0.02453601]],\n\n        ...,\n\n        [[0.02453601],\n         [0.02453601],\n         [0.02453601],\n         ...,\n         [0.02453601],\n         [0.02453601],\n         [0.02453601]],\n\n        [[0.02453601],\n         [0.02453601],\n         [0.02453601],\n         ...,\n         [0.02453601],\n         [0.02453601],\n         [0.02453601]],\n\n        [[0.02453601],\n         [0.02453601],\n         [0.02453601],\n         ...,\n         [0.02453601],\n         [0.02453601],\n         [0.02453601]]]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 291
    }
   ],
   "source": [
    "encDec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596188517751",
   "display_name": "Python 3.8.3 64-bit ('tf': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}