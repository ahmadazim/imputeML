{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "import scipy.sparse as sp_sparse\n",
    "import scanpy as sc\n",
    "from math import log\n",
    "from statistics import median\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "import keras.losses\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read_10x_mtx(\n",
    "    '/home/ahmadazim/data/filtered_gene_bc_matrices/hg19',  # the directory with the `.mtx` file\n",
    "    var_names='gene_symbols',                      # use gene symbols for the variable names (variables-axis index)\n",
    "    cache=True)\n",
    "\n",
    "adata.var_names_make_unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Working on 2700 cells and 32738 genes\n"
    }
   ],
   "source": [
    "data = pd.DataFrame.sparse.from_spmatrix(adata.X)\n",
    "print('Working on {} cells and {} genes'.format(*data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(2700, 16634)"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# Filter out genes that are not expressed in any cells\n",
    "geneSum = data.sum(axis=0)\n",
    "x = geneSum.index[geneSum == 0].tolist()\n",
    "data = data.drop(x, axis = 1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Normalizing data (using method from Rao, et al.)\n",
    "cellSum  = data.sum(axis=1)\n",
    "median_j = median(cellSum)\n",
    "npData = np.asarray(data)\n",
    "for j in range(2700):\n",
    "    cellSum_j = cellSum[j]\n",
    "    for i in range(16634):\n",
    "        npData[j,i] = log( ( (npData[j,i])/(cellSum_j) * median_j ) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "      0      1      2      3      4      5      6      7      8      9      \\\n0       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n1       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n2       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n3       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n4       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n...     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n2695    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n2696    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n2697    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n2698    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n2699    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n\n      ...  16624     16625  16626  16627     16628  16629     16630  16631  \\\n0     ...    0.0  1.532536    0.0    0.0  0.000000    0.0  0.000000    0.0   \n1     ...    0.0  1.522734    0.0    0.0  0.370248    0.0  0.000000    0.0   \n2     ...    0.0  1.332558    0.0    0.0  0.000000    0.0  0.000000    0.0   \n3     ...    0.0  0.980213    0.0    0.0  0.000000    0.0  0.000000    0.0   \n4     ...    0.0  1.175435    0.0    0.0  0.000000    0.0  0.000000    0.0   \n...   ...    ...       ...    ...    ...       ...    ...       ...    ...   \n2695  ...    0.0  0.491513    0.0    0.0  0.000000    0.0  0.000000    0.0   \n2696  ...    0.0  1.266796    0.0    0.0  0.000000    0.0  0.000000    0.0   \n2697  ...    0.0  1.827533    0.0    0.0  0.000000    0.0  0.000000    0.0   \n2698  ...    0.0  1.145975    0.0    0.0  0.000000    0.0  1.145975    0.0   \n2699  ...    0.0  1.463349    0.0    0.0  0.000000    0.0  0.000000    0.0   \n\n      16632  16633  \n0       0.0    0.0  \n1       0.0    0.0  \n2       0.0    0.0  \n3       0.0    0.0  \n4       0.0    0.0  \n...     ...    ...  \n2695    0.0    0.0  \n2696    0.0    0.0  \n2697    0.0    0.0  \n2698    0.0    0.0  \n2699    0.0    0.0  \n\n[2700 rows x 16634 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>16624</th>\n      <th>16625</th>\n      <th>16626</th>\n      <th>16627</th>\n      <th>16628</th>\n      <th>16629</th>\n      <th>16630</th>\n      <th>16631</th>\n      <th>16632</th>\n      <th>16633</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.532536</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.522734</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.370248</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.332558</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.980213</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.175435</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2695</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.491513</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2696</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.266796</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2697</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.827533</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2698</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.145975</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>1.145975</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2699</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.463349</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2700 rows × 16634 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "dataNorm = pd.DataFrame(npData)\n",
    "dataNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Autoencoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Info from TF tutorial below, but model not actually implemented)\n",
    "- Use 2 small ConvNets for the encoder and decoder networks (inference/recognition and generative models)\n",
    "- *x* = observation and *z* = latent variable\n",
    "- Encoder network defines the approximate posterior distribution *q(z|x)*\n",
    "    - Use 2 convolutional layers followed by a fully-connected layer\n",
    "- Decoder network defines the approximate conditional distribution *p(x|z)*\n",
    "    - Use a fully-connected layer followed by three convolution transpose layers\n",
    "- Reparameterization Trick...\n",
    "    - Backpropagation cannot flow through random node\n",
    "    - Approximate *z* using the decoder parameters and another parameter $\\epsilon$ as follows: \n",
    "        - $z = \\mu + \\sigma \\odot \\epsilon$\n",
    "        - where $\\mu$ and $\\sigma$ represent the mean and standard deviation of a Gaussian distribution respectively\n",
    "    - Enables model to backpropagate gradients in the encoder through $\\mu$ and $\\sigma$ while maintaining stochasticity through $\\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Variational AutoEncoder (keras tutorial, but 1 Dimensional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going to try dividing matrix into small \"images\" \n",
    "(2700 x 16634) --> 225 (12 x 72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(2700, 16200)"
     },
     "metadata": {},
     "execution_count": 195
    }
   ],
   "source": [
    "VMR = dataNorm.std() / dataNorm.mean() \n",
    "lowestVMR = VMR.sort_values(ascending=False)[(225*72):]\n",
    "fullData = np.asarray(dataNorm.drop(lowestVMR.index, axis= 1))\n",
    "fullData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Starting i = 1.0\nStarting i = 2.0\nStarting i = 3.0\nStarting i = 4.0\nStarting i = 5.0\nStarting i = 6.0\nStarting i = 7.0\nStarting i = 8.0\nStarting i = 9.0\nStarting i = 10.0\nStarting i = 11.0\nStarting i = 12.0\nStarting i = 13.0\nStarting i = 14.0\nStarting i = 15.0\nStarting i = 16.0\nStarting i = 17.0\nStarting i = 18.0\nStarting i = 19.0\nStarting i = 20.0\nStarting i = 21.0\nStarting i = 22.0\nStarting i = 23.0\nStarting i = 24.0\nStarting i = 25.0\nStarting i = 26.0\nStarting i = 27.0\nStarting i = 28.0\nStarting i = 29.0\nStarting i = 30.0\nStarting i = 31.0\nStarting i = 32.0\nStarting i = 33.0\nStarting i = 34.0\nStarting i = 35.0\nStarting i = 36.0\nStarting i = 37.0\nStarting i = 38.0\nStarting i = 39.0\nStarting i = 40.0\nStarting i = 41.0\nStarting i = 42.0\nStarting i = 43.0\nStarting i = 44.0\nStarting i = 45.0\nStarting i = 46.0\nStarting i = 47.0\nStarting i = 48.0\nStarting i = 49.0\nStarting i = 50.0\nStarting i = 51.0\nStarting i = 52.0\nStarting i = 53.0\nStarting i = 54.0\nStarting i = 55.0\nStarting i = 56.0\nStarting i = 57.0\nStarting i = 58.0\nStarting i = 59.0\nStarting i = 60.0\nStarting i = 61.0\nStarting i = 62.0\nStarting i = 63.0\nStarting i = 64.0\nStarting i = 65.0\nStarting i = 66.0\nStarting i = 67.0\nStarting i = 68.0\nStarting i = 69.0\nStarting i = 70.0\nStarting i = 71.0\nStarting i = 72.0\nStarting i = 73.0\nStarting i = 74.0\nStarting i = 75.0\nStarting i = 76.0\nStarting i = 77.0\nStarting i = 78.0\nStarting i = 79.0\nStarting i = 80.0\nStarting i = 81.0\nStarting i = 82.0\nStarting i = 83.0\nStarting i = 84.0\nStarting i = 85.0\nStarting i = 86.0\nStarting i = 87.0\nStarting i = 88.0\nStarting i = 89.0\nStarting i = 90.0\nStarting i = 91.0\nStarting i = 92.0\nStarting i = 93.0\nStarting i = 94.0\nStarting i = 95.0\nStarting i = 96.0\nStarting i = 97.0\nStarting i = 98.0\nStarting i = 99.0\nStarting i = 100.0\nStarting i = 101.0\nStarting i = 102.0\nStarting i = 103.0\nStarting i = 104.0\nStarting i = 105.0\nStarting i = 106.0\nStarting i = 107.0\nStarting i = 108.0\nStarting i = 109.0\nStarting i = 110.0\nStarting i = 111.0\nStarting i = 112.0\nStarting i = 113.0\nStarting i = 114.0\nStarting i = 115.0\nStarting i = 116.0\nStarting i = 117.0\nStarting i = 118.0\nStarting i = 119.0\nStarting i = 120.0\nStarting i = 121.0\nStarting i = 122.0\nStarting i = 123.0\nStarting i = 124.0\nStarting i = 125.0\nStarting i = 126.0\nStarting i = 127.0\nStarting i = 128.0\nStarting i = 129.0\nStarting i = 130.0\nStarting i = 131.0\nStarting i = 132.0\nStarting i = 133.0\nStarting i = 134.0\nStarting i = 135.0\nStarting i = 136.0\nStarting i = 137.0\nStarting i = 138.0\nStarting i = 139.0\nStarting i = 140.0\nStarting i = 141.0\nStarting i = 142.0\nStarting i = 143.0\nStarting i = 144.0\nStarting i = 145.0\nStarting i = 146.0\nStarting i = 147.0\nStarting i = 148.0\nStarting i = 149.0\nStarting i = 150.0\nStarting i = 151.0\nStarting i = 152.0\nStarting i = 153.0\nStarting i = 154.0\nStarting i = 155.0\nStarting i = 156.0\nStarting i = 157.0\nStarting i = 158.0\nStarting i = 159.0\nStarting i = 160.0\nStarting i = 161.0\nStarting i = 162.0\nStarting i = 163.0\nStarting i = 164.0\nStarting i = 165.0\nStarting i = 166.0\nStarting i = 167.0\nStarting i = 168.0\nStarting i = 169.0\nStarting i = 170.0\nStarting i = 171.0\nStarting i = 172.0\nStarting i = 173.0\nStarting i = 174.0\nStarting i = 175.0\nStarting i = 176.0\nStarting i = 177.0\nStarting i = 178.0\nStarting i = 179.0\nStarting i = 180.0\nStarting i = 181.0\nStarting i = 182.0\nStarting i = 183.0\nStarting i = 184.0\nStarting i = 185.0\nStarting i = 186.0\nStarting i = 187.0\nStarting i = 188.0\nStarting i = 189.0\nStarting i = 190.0\nStarting i = 191.0\nStarting i = 192.0\nStarting i = 193.0\nStarting i = 194.0\nStarting i = 195.0\nStarting i = 196.0\nStarting i = 197.0\nStarting i = 198.0\nStarting i = 199.0\nStarting i = 200.0\nStarting i = 201.0\nStarting i = 202.0\nStarting i = 203.0\nStarting i = 204.0\nStarting i = 205.0\nStarting i = 206.0\nStarting i = 207.0\nStarting i = 208.0\nStarting i = 209.0\nStarting i = 210.0\nStarting i = 211.0\nStarting i = 212.0\nStarting i = 213.0\nStarting i = 214.0\nStarting i = 215.0\nStarting i = 216.0\nStarting i = 217.0\nStarting i = 218.0\nStarting i = 219.0\nStarting i = 220.0\nStarting i = 221.0\nStarting i = 222.0\nStarting i = 223.0\nStarting i = 224.0\nStarting i = 225.0\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(50625, 12, 72, 1)"
     },
     "metadata": {},
     "execution_count": 241
    }
   ],
   "source": [
    "# \"Chunking\" the matrix \n",
    "reshapeData = np.array([[[0 for x in range(72)] for x in range(12)] for x in range(50625)], dtype = 'float32')\n",
    "for i in range(0, 2700, 12):   # i refers to cells (12)\n",
    "    print(\"Starting i =\", (i/12)+1)\n",
    "    for j in range(0, 16200, 72):   # j refers to genes (72)\n",
    "        x = ( 225*(i/12) ) + ( (j/72)+1 )\n",
    "        smallMat = fullData[ i:(i+12) , j:(j+72) ]\n",
    "        reshapeData[int(x-1),:,:] = smallMat\n",
    "\n",
    "reshapeData = reshapeData.reshape((reshapeData.shape[0], reshapeData.shape[1], reshapeData.shape[2], 1))\n",
    "reshapeData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sampling layer\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"encoder\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_16 (InputLayer)           [(None, 12, 72, 1)]  0                                            \n__________________________________________________________________________________________________\nconv2d_14 (Conv2D)              (None, 6, 36, 32)    320         input_16[0][0]                   \n__________________________________________________________________________________________________\nconv2d_15 (Conv2D)              (None, 3, 18, 64)    18496       conv2d_14[0][0]                  \n__________________________________________________________________________________________________\nflatten_8 (Flatten)             (None, 3456)         0           conv2d_15[0][0]                  \n__________________________________________________________________________________________________\ndense_15 (Dense)                (None, 16)           55312       flatten_8[0][0]                  \n__________________________________________________________________________________________________\nz_mean (Dense)                  (None, 32)           544         dense_15[0][0]                   \n__________________________________________________________________________________________________\nz_log_var (Dense)               (None, 32)           544         dense_15[0][0]                   \n__________________________________________________________________________________________________\nsampling_8 (Sampling)           (None, 32)           0           z_mean[0][0]                     \n                                                                 z_log_var[0][0]                  \n==================================================================================================\nTotal params: 75,216\nTrainable params: 75,216\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
    }
   ],
   "source": [
    "# Build the encoder\n",
    "latent_dim = 32\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(12, 72, 1))\n",
    "x = layers.Conv2D(filters= 32, kernel_size= 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "x = layers.Conv2D(filters= 64, kernel_size= 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"decoder\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_18 (InputLayer)        [(None, 32)]              0         \n_________________________________________________________________\ndense_17 (Dense)             (None, 3456)              114048    \n_________________________________________________________________\nreshape_8 (Reshape)          (None, 3, 18, 64)         0         \n_________________________________________________________________\nconv2d_transpose_21 (Conv2DT (None, 6, 36, 64)         36928     \n_________________________________________________________________\nconv2d_transpose_22 (Conv2DT (None, 12, 72, 32)        18464     \n_________________________________________________________________\nconv2d_transpose_23 (Conv2DT (None, 12, 72, 1)         289       \n=================================================================\nTotal params: 169,729\nTrainable params: 169,729\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "# Build the decoder\n",
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(3 * 18 * 64, activation=\"relu\")(latent_inputs)\n",
    "x = layers.Reshape((3, 18, 64))(x)\n",
    "x = layers.Conv2DTranspose(filters= 64, kernel_size= 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Conv2DTranspose(filters= 32, kernel_size= 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the VAE as a Model with a custom train_step\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def train_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = encoder(data)\n",
    "            reconstruction = decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                keras.losses.mean_squared_error(data, reconstruction)\n",
    "            )\n",
    "            reconstruction_loss *= 12 * 72\n",
    "            kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "            kl_loss = tf.reduce_mean(kl_loss)\n",
    "            kl_loss *= -0.5\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"reconstruction_loss\": reconstruction_loss,\n",
    "            \"kl_loss\": kl_loss,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/50\n792/792 [==============================] - 32s 41ms/step - loss: 18.0740 - reconstruction_loss: 17.6169 - kl_loss: 0.4571\nEpoch 2/50\n792/792 [==============================] - 34s 43ms/step - loss: 17.6948 - reconstruction_loss: 17.3001 - kl_loss: 0.3946\nEpoch 3/50\n792/792 [==============================] - 36s 45ms/step - loss: 17.5447 - reconstruction_loss: 17.1862 - kl_loss: 0.3585\nEpoch 4/50\n792/792 [==============================] - 40s 51ms/step - loss: 17.4276 - reconstruction_loss: 17.1040 - kl_loss: 0.3236\nEpoch 5/50\n792/792 [==============================] - 36s 46ms/step - loss: 17.3525 - reconstruction_loss: 17.0408 - kl_loss: 0.3117\nEpoch 6/50\n792/792 [==============================] - 36s 45ms/step - loss: 17.3350 - reconstruction_loss: 17.0348 - kl_loss: 0.3002\nEpoch 7/50\n792/792 [==============================] - 37s 47ms/step - loss: 17.3511 - reconstruction_loss: 17.0646 - kl_loss: 0.2865\nEpoch 8/50\n792/792 [==============================] - 34s 43ms/step - loss: 17.3766 - reconstruction_loss: 17.1070 - kl_loss: 0.2696\nEpoch 9/50\n792/792 [==============================] - 36s 45ms/step - loss: 17.2904 - reconstruction_loss: 17.0410 - kl_loss: 0.2494\nEpoch 10/50\n792/792 [==============================] - 36s 46ms/step - loss: 17.2630 - reconstruction_loss: 17.0376 - kl_loss: 0.2254\nEpoch 11/50\n792/792 [==============================] - 37s 47ms/step - loss: 17.2469 - reconstruction_loss: 17.0499 - kl_loss: 0.1971\nEpoch 12/50\n792/792 [==============================] - 36s 45ms/step - loss: 17.2295 - reconstruction_loss: 17.0652 - kl_loss: 0.1643\nEpoch 13/50\n792/792 [==============================] - 33s 42ms/step - loss: 17.2013 - reconstruction_loss: 17.0715 - kl_loss: 0.1298\nEpoch 14/50\n792/792 [==============================] - 36s 45ms/step - loss: 17.1901 - reconstruction_loss: 17.0920 - kl_loss: 0.0981\nEpoch 15/50\n792/792 [==============================] - 36s 46ms/step - loss: 17.1452 - reconstruction_loss: 17.0632 - kl_loss: 0.0819\nEpoch 16/50\n792/792 [==============================] - 37s 46ms/step - loss: 17.1347 - reconstruction_loss: 17.0553 - kl_loss: 0.0793\nEpoch 17/50\n792/792 [==============================] - 37s 46ms/step - loss: 17.1084 - reconstruction_loss: 17.0344 - kl_loss: 0.0740\nEpoch 18/50\n792/792 [==============================] - 36s 45ms/step - loss: 17.1076 - reconstruction_loss: 17.0416 - kl_loss: 0.0660\nEpoch 19/50\n792/792 [==============================] - 37s 47ms/step - loss: 17.1282 - reconstruction_loss: 17.0724 - kl_loss: 0.0557\nEpoch 20/50\n792/792 [==============================] - 37s 46ms/step - loss: 17.0979 - reconstruction_loss: 17.0543 - kl_loss: 0.0436\nEpoch 21/50\n792/792 [==============================] - 36s 46ms/step - loss: 17.1032 - reconstruction_loss: 17.0673 - kl_loss: 0.0359\nEpoch 22/50\n792/792 [==============================] - 37s 47ms/step - loss: 17.0987 - reconstruction_loss: 17.0683 - kl_loss: 0.0304\nEpoch 23/50\n792/792 [==============================] - 36s 46ms/step - loss: 17.0646 - reconstruction_loss: 17.0366 - kl_loss: 0.0280\nEpoch 24/50\n792/792 [==============================] - 37s 47ms/step - loss: 17.0523 - reconstruction_loss: 17.0241 - kl_loss: 0.0282\nEpoch 25/50\n792/792 [==============================] - 37s 47ms/step - loss: 17.0751 - reconstruction_loss: 17.0477 - kl_loss: 0.0274\nEpoch 26/50\n792/792 [==============================] - 36s 46ms/step - loss: 17.0521 - reconstruction_loss: 17.0258 - kl_loss: 0.0263\nEpoch 27/50\n792/792 [==============================] - 38s 47ms/step - loss: 17.0749 - reconstruction_loss: 17.0499 - kl_loss: 0.0250\nEpoch 28/50\n792/792 [==============================] - 37s 46ms/step - loss: 17.0747 - reconstruction_loss: 17.0515 - kl_loss: 0.0233\nEpoch 29/50\n792/792 [==============================] - 37s 46ms/step - loss: 17.0418 - reconstruction_loss: 17.0205 - kl_loss: 0.0214\nEpoch 30/50\n792/792 [==============================] - 38s 47ms/step - loss: 17.0452 - reconstruction_loss: 17.0257 - kl_loss: 0.0195\nEpoch 31/50\n792/792 [==============================] - 37s 46ms/step - loss: 17.0733 - reconstruction_loss: 17.0561 - kl_loss: 0.0172\nEpoch 32/50\n792/792 [==============================] - 38s 48ms/step - loss: 17.0613 - reconstruction_loss: 17.0467 - kl_loss: 0.0146\nEpoch 33/50\n792/792 [==============================] - 38s 47ms/step - loss: 17.0360 - reconstruction_loss: 17.0218 - kl_loss: 0.0142\nEpoch 34/50\n792/792 [==============================] - 36s 46ms/step - loss: 17.0593 - reconstruction_loss: 17.0476 - kl_loss: 0.0117\nEpoch 35/50\n792/792 [==============================] - 37s 47ms/step - loss: 17.0665 - reconstruction_loss: 17.0568 - kl_loss: 0.0097\nEpoch 36/50\n792/792 [==============================] - 36s 45ms/step - loss: 17.0552 - reconstruction_loss: 17.0478 - kl_loss: 0.0074\nEpoch 37/50\n792/792 [==============================] - 37s 46ms/step - loss: 17.0322 - reconstruction_loss: 17.0264 - kl_loss: 0.0058\nEpoch 38/50\n792/792 [==============================] - 36s 46ms/step - loss: 17.0767 - reconstruction_loss: 17.0724 - kl_loss: 0.0043\nEpoch 39/50\n792/792 [==============================] - 36s 45ms/step - loss: 17.0557 - reconstruction_loss: 17.0523 - kl_loss: 0.0034\nEpoch 40/50\n792/792 [==============================] - 36s 46ms/step - loss: 17.0359 - reconstruction_loss: 17.0332 - kl_loss: 0.0027\nEpoch 41/50\n792/792 [==============================] - 36s 45ms/step - loss: 17.0418 - reconstruction_loss: 17.0398 - kl_loss: 0.0021\nEpoch 42/50\n792/792 [==============================] - 36s 45ms/step - loss: 17.0249 - reconstruction_loss: 17.0237 - kl_loss: 0.0011\nEpoch 43/50\n792/792 [==============================] - 35s 45ms/step - loss: 17.0653 - reconstruction_loss: 17.0645 - kl_loss: 7.3957e-04\nEpoch 44/50\n792/792 [==============================] - 35s 45ms/step - loss: 17.0401 - reconstruction_loss: 17.0398 - kl_loss: 2.5749e-04\nEpoch 45/50\n792/792 [==============================] - 35s 44ms/step - loss: 17.0298 - reconstruction_loss: 17.0297 - kl_loss: 4.2766e-05\nEpoch 46/50\n792/792 [==============================] - 35s 44ms/step - loss: 17.0346 - reconstruction_loss: 17.0346 - kl_loss: 3.4714e-06\nEpoch 47/50\n792/792 [==============================] - 35s 44ms/step - loss: 17.0290 - reconstruction_loss: 17.0290 - kl_loss: 9.4074e-08\nEpoch 48/50\n792/792 [==============================] - 35s 44ms/step - loss: 17.0684 - reconstruction_loss: 17.0684 - kl_loss: 4.5803e-10\nEpoch 49/50\n792/792 [==============================] - 36s 45ms/step - loss: 17.0336 - reconstruction_loss: 17.0336 - kl_loss: 0.0000e+00\nEpoch 50/50\n792/792 [==============================] - 35s 44ms/step - loss: 17.0312 - reconstruction_loss: 17.0312 - kl_loss: 0.0000e+00\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7fbaddac2130>"
     },
     "metadata": {},
     "execution_count": 287
    }
   ],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())\n",
    "vae.fit(reshapeData, epochs=50, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encDec = decoder.predict(encoder.predict(reshapeData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(50625, 12, 72, 1)"
     },
     "metadata": {},
     "execution_count": 302
    }
   ],
   "source": [
    "encDec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Starting run = 100.0\nStarting run = 200.0\nStarting run = 300.0\nStarting run = 400.0\nStarting run = 500.0\nStarting run = 600.0\nStarting run = 700.0\nStarting run = 800.0\nStarting run = 900.0\nStarting run = 1000.0\nStarting run = 1100.0\nStarting run = 1200.0\nStarting run = 1300.0\nStarting run = 1400.0\nStarting run = 1500.0\nStarting run = 1600.0\nStarting run = 1700.0\nStarting run = 1800.0\nStarting run = 1900.0\nStarting run = 2000.0\nStarting run = 2100.0\nStarting run = 2200.0\nStarting run = 2300.0\nStarting run = 2400.0\nStarting run = 2500.0\nStarting run = 2600.0\nStarting run = 2700.0\nStarting run = 2800.0\nStarting run = 2900.0\nStarting run = 3000.0\nStarting run = 3100.0\nStarting run = 3200.0\nStarting run = 3300.0\nStarting run = 3400.0\nStarting run = 3500.0\nStarting run = 3600.0\nStarting run = 3700.0\nStarting run = 3800.0\nStarting run = 3900.0\nStarting run = 4000.0\nStarting run = 4100.0\nStarting run = 4200.0\nStarting run = 4300.0\nStarting run = 4400.0\nStarting run = 4500.0\nStarting run = 4600.0\nStarting run = 4700.0\nStarting run = 4800.0\nStarting run = 4900.0\nStarting run = 5000.0\nStarting run = 5100.0\nStarting run = 5200.0\nStarting run = 5300.0\nStarting run = 5400.0\nStarting run = 5500.0\nStarting run = 5600.0\nStarting run = 5700.0\nStarting run = 5800.0\nStarting run = 5900.0\nStarting run = 6000.0\nStarting run = 6100.0\nStarting run = 6200.0\nStarting run = 6300.0\nStarting run = 6400.0\nStarting run = 6500.0\nStarting run = 6600.0\nStarting run = 6700.0\nStarting run = 6800.0\nStarting run = 6900.0\nStarting run = 7000.0\nStarting run = 7100.0\nStarting run = 7200.0\nStarting run = 7300.0\nStarting run = 7400.0\nStarting run = 7500.0\nStarting run = 7600.0\nStarting run = 7700.0\nStarting run = 7800.0\nStarting run = 7900.0\nStarting run = 8000.0\nStarting run = 8100.0\nStarting run = 8200.0\nStarting run = 8300.0\nStarting run = 8400.0\nStarting run = 8500.0\nStarting run = 8600.0\nStarting run = 8700.0\nStarting run = 8800.0\nStarting run = 8900.0\nStarting run = 9000.0\nStarting run = 9100.0\nStarting run = 9200.0\nStarting run = 9300.0\nStarting run = 9400.0\nStarting run = 9500.0\nStarting run = 9600.0\nStarting run = 9700.0\nStarting run = 9800.0\nStarting run = 9900.0\nStarting run = 10000.0\nStarting run = 10100.0\nStarting run = 10200.0\nStarting run = 10300.0\nStarting run = 10400.0\nStarting run = 10500.0\nStarting run = 10600.0\nStarting run = 10700.0\nStarting run = 10800.0\nStarting run = 10900.0\nStarting run = 11000.0\nStarting run = 11100.0\nStarting run = 11200.0\nStarting run = 11300.0\nStarting run = 11400.0\nStarting run = 11500.0\nStarting run = 11600.0\nStarting run = 11700.0\nStarting run = 11800.0\nStarting run = 11900.0\nStarting run = 12000.0\nStarting run = 12100.0\nStarting run = 12200.0\nStarting run = 12300.0\nStarting run = 12400.0\nStarting run = 12500.0\nStarting run = 12600.0\nStarting run = 12700.0\nStarting run = 12800.0\nStarting run = 12900.0\nStarting run = 13000.0\nStarting run = 13100.0\nStarting run = 13200.0\nStarting run = 13300.0\nStarting run = 13400.0\nStarting run = 13500.0\nStarting run = 13600.0\nStarting run = 13700.0\nStarting run = 13800.0\nStarting run = 13900.0\nStarting run = 14000.0\nStarting run = 14100.0\nStarting run = 14200.0\nStarting run = 14300.0\nStarting run = 14400.0\nStarting run = 14500.0\nStarting run = 14600.0\nStarting run = 14700.0\nStarting run = 14800.0\nStarting run = 14900.0\nStarting run = 15000.0\nStarting run = 15100.0\nStarting run = 15200.0\nStarting run = 15300.0\nStarting run = 15400.0\nStarting run = 15500.0\nStarting run = 15600.0\nStarting run = 15700.0\nStarting run = 15800.0\nStarting run = 15900.0\nStarting run = 16000.0\nStarting run = 16100.0\nStarting run = 16200.0\nStarting run = 16300.0\nStarting run = 16400.0\nStarting run = 16500.0\nStarting run = 16600.0\nStarting run = 16700.0\nStarting run = 16800.0\nStarting run = 16900.0\nStarting run = 17000.0\nStarting run = 17100.0\nStarting run = 17200.0\nStarting run = 17300.0\nStarting run = 17400.0\nStarting run = 17500.0\nStarting run = 17600.0\nStarting run = 17700.0\nStarting run = 17800.0\nStarting run = 17900.0\nStarting run = 18000.0\nStarting run = 18100.0\nStarting run = 18200.0\nStarting run = 18300.0\nStarting run = 18400.0\nStarting run = 18500.0\nStarting run = 18600.0\nStarting run = 18700.0\nStarting run = 18800.0\nStarting run = 18900.0\nStarting run = 19000.0\nStarting run = 19100.0\nStarting run = 19200.0\nStarting run = 19300.0\nStarting run = 19400.0\nStarting run = 19500.0\nStarting run = 19600.0\nStarting run = 19700.0\nStarting run = 19800.0\nStarting run = 19900.0\nStarting run = 20000.0\nStarting run = 20100.0\nStarting run = 20200.0\nStarting run = 20300.0\nStarting run = 20400.0\nStarting run = 20500.0\nStarting run = 20600.0\nStarting run = 20700.0\nStarting run = 20800.0\nStarting run = 20900.0\nStarting run = 21000.0\nStarting run = 21100.0\nStarting run = 21200.0\nStarting run = 21300.0\nStarting run = 21400.0\nStarting run = 21500.0\nStarting run = 21600.0\nStarting run = 21700.0\nStarting run = 21800.0\nStarting run = 21900.0\nStarting run = 22000.0\nStarting run = 22100.0\nStarting run = 22200.0\nStarting run = 22300.0\nStarting run = 22400.0\nStarting run = 22500.0\nStarting run = 22600.0\nStarting run = 22700.0\nStarting run = 22800.0\nStarting run = 22900.0\nStarting run = 23000.0\nStarting run = 23100.0\nStarting run = 23200.0\nStarting run = 23300.0\nStarting run = 23400.0\nStarting run = 23500.0\nStarting run = 23600.0\nStarting run = 23700.0\nStarting run = 23800.0\nStarting run = 23900.0\nStarting run = 24000.0\nStarting run = 24100.0\nStarting run = 24200.0\nStarting run = 24300.0\nStarting run = 24400.0\nStarting run = 24500.0\nStarting run = 24600.0\nStarting run = 24700.0\nStarting run = 24800.0\nStarting run = 24900.0\nStarting run = 25000.0\nStarting run = 25100.0\nStarting run = 25200.0\nStarting run = 25300.0\nStarting run = 25400.0\nStarting run = 25500.0\nStarting run = 25600.0\nStarting run = 25700.0\nStarting run = 25800.0\nStarting run = 25900.0\nStarting run = 26000.0\nStarting run = 26100.0\nStarting run = 26200.0\nStarting run = 26300.0\nStarting run = 26400.0\nStarting run = 26500.0\nStarting run = 26600.0\nStarting run = 26700.0\nStarting run = 26800.0\nStarting run = 26900.0\nStarting run = 27000.0\nStarting run = 27100.0\nStarting run = 27200.0\nStarting run = 27300.0\nStarting run = 27400.0\nStarting run = 27500.0\nStarting run = 27600.0\nStarting run = 27700.0\nStarting run = 27800.0\nStarting run = 27900.0\nStarting run = 28000.0\nStarting run = 28100.0\nStarting run = 28200.0\nStarting run = 28300.0\nStarting run = 28400.0\nStarting run = 28500.0\nStarting run = 28600.0\nStarting run = 28700.0\nStarting run = 28800.0\nStarting run = 28900.0\nStarting run = 29000.0\nStarting run = 29100.0\nStarting run = 29200.0\nStarting run = 29300.0\nStarting run = 29400.0\nStarting run = 29500.0\nStarting run = 29600.0\nStarting run = 29700.0\nStarting run = 29800.0\nStarting run = 29900.0\nStarting run = 30000.0\nStarting run = 30100.0\nStarting run = 30200.0\nStarting run = 30300.0\nStarting run = 30400.0\nStarting run = 30500.0\nStarting run = 30600.0\nStarting run = 30700.0\nStarting run = 30800.0\nStarting run = 30900.0\nStarting run = 31000.0\nStarting run = 31100.0\nStarting run = 31200.0\nStarting run = 31300.0\nStarting run = 31400.0\nStarting run = 31500.0\nStarting run = 31600.0\nStarting run = 31700.0\nStarting run = 31800.0\nStarting run = 31900.0\nStarting run = 32000.0\nStarting run = 32100.0\nStarting run = 32200.0\nStarting run = 32300.0\nStarting run = 32400.0\nStarting run = 32500.0\nStarting run = 32600.0\nStarting run = 32700.0\nStarting run = 32800.0\nStarting run = 32900.0\nStarting run = 33000.0\nStarting run = 33100.0\nStarting run = 33200.0\nStarting run = 33300.0\nStarting run = 33400.0\nStarting run = 33500.0\nStarting run = 33600.0\nStarting run = 33700.0\nStarting run = 33800.0\nStarting run = 33900.0\nStarting run = 34000.0\nStarting run = 34100.0\nStarting run = 34200.0\nStarting run = 34300.0\nStarting run = 34400.0\nStarting run = 34500.0\nStarting run = 34600.0\nStarting run = 34700.0\nStarting run = 34800.0\nStarting run = 34900.0\nStarting run = 35000.0\nStarting run = 35100.0\nStarting run = 35200.0\nStarting run = 35300.0\nStarting run = 35400.0\nStarting run = 35500.0\nStarting run = 35600.0\nStarting run = 35700.0\nStarting run = 35800.0\nStarting run = 35900.0\nStarting run = 36000.0\nStarting run = 36100.0\nStarting run = 36200.0\nStarting run = 36300.0\nStarting run = 36400.0\nStarting run = 36500.0\nStarting run = 36600.0\nStarting run = 36700.0\nStarting run = 36800.0\nStarting run = 36900.0\nStarting run = 37000.0\nStarting run = 37100.0\nStarting run = 37200.0\nStarting run = 37300.0\nStarting run = 37400.0\nStarting run = 37500.0\nStarting run = 37600.0\nStarting run = 37700.0\nStarting run = 37800.0\nStarting run = 37900.0\nStarting run = 38000.0\nStarting run = 38100.0\nStarting run = 38200.0\nStarting run = 38300.0\nStarting run = 38400.0\nStarting run = 38500.0\nStarting run = 38600.0\nStarting run = 38700.0\nStarting run = 38800.0\nStarting run = 38900.0\nStarting run = 39000.0\nStarting run = 39100.0\nStarting run = 39200.0\nStarting run = 39300.0\nStarting run = 39400.0\nStarting run = 39500.0\nStarting run = 39600.0\nStarting run = 39700.0\nStarting run = 39800.0\nStarting run = 39900.0\nStarting run = 40000.0\nStarting run = 40100.0\nStarting run = 40200.0\nStarting run = 40300.0\nStarting run = 40400.0\nStarting run = 40500.0\nStarting run = 40600.0\nStarting run = 40700.0\nStarting run = 40800.0\nStarting run = 40900.0\nStarting run = 41000.0\nStarting run = 41100.0\nStarting run = 41200.0\nStarting run = 41300.0\nStarting run = 41400.0\nStarting run = 41500.0\nStarting run = 41600.0\nStarting run = 41700.0\nStarting run = 41800.0\nStarting run = 41900.0\nStarting run = 42000.0\nStarting run = 42100.0\nStarting run = 42200.0\nStarting run = 42300.0\nStarting run = 42400.0\nStarting run = 42500.0\nStarting run = 42600.0\nStarting run = 42700.0\nStarting run = 42800.0\nStarting run = 42900.0\nStarting run = 43000.0\nStarting run = 43100.0\nStarting run = 43200.0\nStarting run = 43300.0\nStarting run = 43400.0\nStarting run = 43500.0\nStarting run = 43600.0\nStarting run = 43700.0\nStarting run = 43800.0\nStarting run = 43900.0\nStarting run = 44000.0\nStarting run = 44100.0\nStarting run = 44200.0\nStarting run = 44300.0\nStarting run = 44400.0\nStarting run = 44500.0\nStarting run = 44600.0\nStarting run = 44700.0\nStarting run = 44800.0\nStarting run = 44900.0\nStarting run = 45000.0\nStarting run = 45100.0\nStarting run = 45200.0\nStarting run = 45300.0\nStarting run = 45400.0\nStarting run = 45500.0\nStarting run = 45600.0\nStarting run = 45700.0\nStarting run = 45800.0\nStarting run = 45900.0\nStarting run = 46000.0\nStarting run = 46100.0\nStarting run = 46200.0\nStarting run = 46300.0\nStarting run = 46400.0\nStarting run = 46500.0\nStarting run = 46600.0\nStarting run = 46700.0\nStarting run = 46800.0\nStarting run = 46900.0\nStarting run = 47000.0\nStarting run = 47100.0\nStarting run = 47200.0\nStarting run = 47300.0\nStarting run = 47400.0\nStarting run = 47500.0\nStarting run = 47600.0\nStarting run = 47700.0\nStarting run = 47800.0\nStarting run = 47900.0\nStarting run = 48000.0\nStarting run = 48100.0\nStarting run = 48200.0\nStarting run = 48300.0\nStarting run = 48400.0\nStarting run = 48500.0\nStarting run = 48600.0\nStarting run = 48700.0\nStarting run = 48800.0\nStarting run = 48900.0\nStarting run = 49000.0\nStarting run = 49100.0\nStarting run = 49200.0\nStarting run = 49300.0\nStarting run = 49400.0\nStarting run = 49500.0\nStarting run = 49600.0\nStarting run = 49700.0\nStarting run = 49800.0\nStarting run = 49900.0\nStarting run = 50000.0\nStarting run = 50100.0\nStarting run = 50200.0\nStarting run = 50300.0\nStarting run = 50400.0\nStarting run = 50500.0\nStarting run = 50600.0\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(2700, 16200)"
     },
     "metadata": {},
     "execution_count": 314
    }
   ],
   "source": [
    "# Putting the matrix back together\n",
    "result = np.array([[0 for x in range(16200)] for x in range(2700)], dtype = 'float32')\n",
    "encDec = encDec.reshape(reshapeData.shape[0], reshapeData.shape[1], reshapeData.shape[2])\n",
    "\n",
    "for i in range(0, 2700, 12):   # cells\n",
    "    for j in range(0, 16200, 72):   # genes\n",
    "        x = ( 225*(i/12) ) + ( (j/72)+1 )\n",
    "        if x%100 == 0:\n",
    "            print(\"Starting run =\", x)\n",
    "        sect = encDec[int(x-1),:,:]\n",
    "        result[i:(i+12) , j:(j+72)] = sect\n",
    "    \n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "         0         1         2         3         4         5         6      \\\n0     0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536   \n1     0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536   \n2     0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536   \n3     0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536   \n4     0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536   \n...        ...       ...       ...       ...       ...       ...       ...   \n2695  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536   \n2696  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536   \n2697  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536   \n2698  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536   \n2699  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536   \n\n         7         8         9      ...     16190     16191     16192  \\\n0     0.024536  0.024536  0.024536  ...  0.024536  0.024536  0.024536   \n1     0.024536  0.024536  0.024536  ...  0.024536  0.024536  0.024536   \n2     0.024536  0.024536  0.024536  ...  0.024536  0.024536  0.024536   \n3     0.024536  0.024536  0.024536  ...  0.024536  0.024536  0.024536   \n4     0.024536  0.024536  0.024536  ...  0.024536  0.024536  0.024536   \n...        ...       ...       ...  ...       ...       ...       ...   \n2695  0.024536  0.024536  0.024536  ...  0.024536  0.024536  0.024536   \n2696  0.024536  0.024536  0.024536  ...  0.024536  0.024536  0.024536   \n2697  0.024536  0.024536  0.024536  ...  0.024536  0.024536  0.024536   \n2698  0.024536  0.024536  0.024536  ...  0.024536  0.024536  0.024536   \n2699  0.024536  0.024536  0.024536  ...  0.024536  0.024536  0.024536   \n\n         16193     16194     16195     16196     16197     16198     16199  \n0     0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  \n1     0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  \n2     0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  \n3     0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  \n4     0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  \n...        ...       ...       ...       ...       ...       ...       ...  \n2695  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  \n2696  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  \n2697  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  \n2698  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  \n2699  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  0.024536  \n\n[2700 rows x 16200 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>16190</th>\n      <th>16191</th>\n      <th>16192</th>\n      <th>16193</th>\n      <th>16194</th>\n      <th>16195</th>\n      <th>16196</th>\n      <th>16197</th>\n      <th>16198</th>\n      <th>16199</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>...</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>...</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>...</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>...</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>...</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2695</th>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>...</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n    </tr>\n    <tr>\n      <th>2696</th>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>...</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n    </tr>\n    <tr>\n      <th>2697</th>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>...</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n    </tr>\n    <tr>\n      <th>2698</th>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>...</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n    </tr>\n    <tr>\n      <th>2699</th>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>...</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n      <td>0.024536</td>\n    </tr>\n  </tbody>\n</table>\n<p>2700 rows × 16200 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 332
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0"
     },
     "metadata": {},
     "execution_count": 332
    }
   ],
   "source": [
    "result_df = pd.DataFrame(result)\n",
    "result_df\n",
    "\n",
    "sum(result_df.sum(axis = 1) != 397.483429)\n",
    "# Outputed matrix filled with all 0.024536\n",
    "# something went wrong..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying Autoencoder with Dense Layers (not Conv.)\n",
    "Since the conv failed so bad. Want to see if beacause of Conv or not..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_9 (Dense)              (None, 8192)              136273920 \n_________________________________________________________________\ndense_10 (Dense)             (None, 4096)              33558528  \n_________________________________________________________________\ndense_11 (Dense)             (None, 1024)              4195328   \n_________________________________________________________________\ndense_12 (Dense)             (None, 512)               524800    \n_________________________________________________________________\ndense_13 (Dense)             (None, 32)                16416     \n_________________________________________________________________\ndense_14 (Dense)             (None, 512)               16896     \n_________________________________________________________________\ndense_15 (Dense)             (None, 1024)              525312    \n_________________________________________________________________\ndense_16 (Dense)             (None, 4096)              4198400   \n_________________________________________________________________\ndense_17 (Dense)             (None, 8192)              33562624  \n_________________________________________________________________\ndense_18 (Dense)             (None, 16634)             136282362 \n=================================================================\nTotal params: 349,154,586\nTrainable params: 349,154,586\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "fullData = np.asarray(dataNorm)\n",
    "n_genes = fullData.shape[1]\n",
    "\n",
    "#add model layers\n",
    "model.add(layers.Input(shape= n_genes))\n",
    "model.add(layers.Dense(8192, activation='relu'))\n",
    "model.add(layers.Dense(4096, activation='relu'))\n",
    "model.add(layers.Dense(1024, activation='relu'))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1024, activation='relu'))\n",
    "model.add(layers.Dense(4096, activation='relu'))\n",
    "model.add(layers.Dense(8192, activation='relu'))\n",
    "model.add(layers.Dense(n_genes, activation='relu'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split (random train = 90%)\n",
    "# trainProp = 0.9\n",
    "# n_cells = fullData.shape[0]\n",
    "\n",
    "# train_idx = np.random.choice(n_cells, int(n_cells * trainProp), replace= False)\n",
    "# test_idx = np.array(list(set(range(n_cells)) - set(train_idx)))\n",
    "# trainAE = fullData[train_idx, :]\n",
    "# testAE = fullData[test_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def nzMSE(y_true, y_pred): \n",
    "    omega = tf.sign(y_true)  # 0 if 0, 1 if > 0\n",
    "    mse_nz = tf.reduce_mean(tf.multiply(tf.pow( (y_pred - y_true), 2), omega))\n",
    "    return mse_nz\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss= nzMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "AEresults = model.fit(x= fullData, \n",
    "                      y= fullData, \n",
    "                      epochs= 100, \n",
    "                      batch_size= 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = pd.DataFrame(model.predict(fullData))\n",
    "predicted\n",
    "# Notes on using MSE...\n",
    "    # At least it's not all zeros this time, but still needs some work\n",
    "    # Need to build a loss function to penalize for zeros \n",
    "\n",
    "# Now added loss function described in paper (nonzero MSE)...\n",
    "    # Did pretty well!!\n",
    "    # 95% zeros --> 28% zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.eq(0).values.sum()/(2700*16634)\n",
    "predicted.eq(0).values.sum()/(2700*16634)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing dropout layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_4\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndropout_16 (Dropout)         (None, 16634)             0         \n_________________________________________________________________\ndense_40 (Dense)             (None, 8192)              136273920 \n_________________________________________________________________\ndropout_17 (Dropout)         (None, 8192)              0         \n_________________________________________________________________\ndense_41 (Dense)             (None, 4096)              33558528  \n_________________________________________________________________\ndropout_18 (Dropout)         (None, 4096)              0         \n_________________________________________________________________\ndense_42 (Dense)             (None, 256)               1048832   \n_________________________________________________________________\ndense_43 (Dense)             (None, 32)                8224      \n_________________________________________________________________\ndense_44 (Dense)             (None, 256)               8448      \n_________________________________________________________________\ndropout_19 (Dropout)         (None, 256)               0         \n_________________________________________________________________\ndense_45 (Dense)             (None, 4096)              1052672   \n_________________________________________________________________\ndropout_20 (Dropout)         (None, 4096)              0         \n_________________________________________________________________\ndense_46 (Dense)             (None, 8192)              33562624  \n_________________________________________________________________\ndropout_21 (Dropout)         (None, 8192)              0         \n_________________________________________________________________\ndense_47 (Dense)             (None, 16634)             136282362 \n=================================================================\nTotal params: 341,795,610\nTrainable params: 341,795,610\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "modelD = Sequential()\n",
    "fullData = np.asarray(dataNorm)\n",
    "n_genes = fullData.shape[1]\n",
    "\n",
    "#add model layers\n",
    "modelD.add(layers.Input(shape= n_genes))\n",
    "modelD.add(layers.Dropout(0.2))\n",
    "modelD.add(layers.Dense(8192, activation='relu'))\n",
    "modelD.add(layers.Dropout(0.2))\n",
    "modelD.add(layers.Dense(4096, activation='relu'))\n",
    "modelD.add(layers.Dropout(0.2))\n",
    "modelD.add(layers.Dense(256, activation='relu'))\n",
    "\n",
    "modelD.add(layers.Dense(32, activation='relu'))\n",
    "\n",
    "modelD.add(layers.Dense(256, activation='relu'))\n",
    "modelD.add(layers.Dropout(0.2))\n",
    "modelD.add(layers.Dense(4096, activation='relu'))\n",
    "modelD.add(layers.Dropout(0.2))\n",
    "modelD.add(layers.Dense(8192, activation='relu'))\n",
    "modelD.add(layers.Dropout(0.2))\n",
    "modelD.add(layers.Dense(n_genes, activation='relu'))\n",
    "\n",
    "modelD.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelD.compile(optimizer='adam', \n",
    "               loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/10\n43/43 [==============================] - 48s 1s/step - loss: 0.0426\nEpoch 2/10\n43/43 [==============================] - 50s 1s/step - loss: 0.0284\nEpoch 3/10\n43/43 [==============================] - 48s 1s/step - loss: 0.0283\nEpoch 4/10\n43/43 [==============================] - 48s 1s/step - loss: 0.0281\nEpoch 5/10\n43/43 [==============================] - 48s 1s/step - loss: 0.0271\nEpoch 6/10\n43/43 [==============================] - 48s 1s/step - loss: 0.0263\nEpoch 7/10\n43/43 [==============================] - 48s 1s/step - loss: 0.0261\nEpoch 8/10\n43/43 [==============================] - 48s 1s/step - loss: 0.0260\nEpoch 9/10\n43/43 [==============================] - 48s 1s/step - loss: 0.0257\nEpoch 10/10\n43/43 [==============================] - 48s 1s/step - loss: 0.0254\n"
    }
   ],
   "source": [
    "# Train model\n",
    "dropoutAdded = modelD.fit(x= fullData, \n",
    "                          y= fullData, \n",
    "                          epochs= 10, \n",
    "                          batch_size= 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "      0      1      2      3      4      5      6      7      8      9      \\\n0       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n1       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n2       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n3       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n4       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n...     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n2695    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n2696    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n2697    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n2698    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n2699    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n\n      ...  16624     16625  16626  16627  16628  16629  16630  16631  16632  \\\n0     ...    0.0  1.503495    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n1     ...    0.0  1.360314    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n2     ...    0.0  1.479689    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n3     ...    0.0  1.253325    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n4     ...    0.0  1.250109    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n...   ...    ...       ...    ...    ...    ...    ...    ...    ...    ...   \n2695  ...    0.0  1.214159    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n2696  ...    0.0  1.431963    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n2697  ...    0.0  1.357407    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n2698  ...    0.0  1.258380    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n2699  ...    0.0  1.512299    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n\n      16633  \n0       0.0  \n1       0.0  \n2       0.0  \n3       0.0  \n4       0.0  \n...     ...  \n2695    0.0  \n2696    0.0  \n2697    0.0  \n2698    0.0  \n2699    0.0  \n\n[2700 rows x 16634 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>16624</th>\n      <th>16625</th>\n      <th>16626</th>\n      <th>16627</th>\n      <th>16628</th>\n      <th>16629</th>\n      <th>16630</th>\n      <th>16631</th>\n      <th>16632</th>\n      <th>16633</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.503495</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.360314</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.479689</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.253325</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.250109</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2695</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.214159</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2696</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.431963</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2697</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.357407</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2698</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.258380</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2699</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>1.512299</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2700 rows × 16634 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "predDropout = pd.DataFrame(modelD.predict(fullData))\n",
    "predDropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.9490805534402985"
     },
     "metadata": {},
     "execution_count": 68
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.9433882186864031"
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "data.eq(0).values.sum()/(2700*16634)\n",
    "predDropout.eq(0).values.sum()/(2700*16634)\n",
    "# Actually made it worse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Number of Significant PCs \n",
    "Maybe that gives appropriate size of latent space (coding size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import*\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import register_cmap\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "NumPy covariance matrix: \n[[ 1.00037053e+00 -1.88282065e-03 -2.26857835e-03 ... -1.09819843e-03\n  -1.13005401e-02  3.46964770e-02]\n [-1.88282065e-03  1.00037054e+00 -1.31268066e-03 ... -6.35456933e-04\n  -6.53889713e-03 -5.10513164e-03]\n [-2.26857835e-03 -1.31268066e-03  1.00037046e+00 ... -7.65651175e-04\n   2.42811796e-03 -6.15108567e-03]\n ...\n [-1.09819843e-03 -6.35456933e-04 -7.65651175e-04 ...  1.00037049e+00\n  -3.81396209e-03 -2.97768540e-03]\n [-1.13005401e-02 -6.53889713e-03  2.42811796e-03 ... -3.81396209e-03\n   1.00037052e+00  3.17662551e-03]\n [ 3.46964770e-02 -5.10513164e-03 -6.15108567e-03 ... -2.97768540e-03\n   3.17662551e-03  1.00037051e+00]]\n"
    }
   ],
   "source": [
    "X_std = StandardScaler().fit_transform(fullData)\n",
    "print('NumPy covariance matrix: \\n%s' %np.cov(X_std.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_mat = np.cov(X_std.T)\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "print('Eigenvectors \\n%s' %eig_vecs)\n",
    "print('\\nEigenvalues \\n%s' %eig_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visually confirm that the list is correctly sorted by decreasing eigenvalues\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "print('Eigenvalues in descending order:')\n",
    "for i in eig_pairs:\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=)\n",
    "pca.fit_transform(df1)\n",
    "print pca.explained_variance_ratio_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explained variance\n",
    "pca = PCA().fit(X_std)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Variational Autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Sampling(layers.Layer): \n",
    "#     def call(self, inputs):\n",
    "#         mean, log_var = inputs\n",
    "#         return K.random_normal(tf.shape(log_var)) * K.exp(log_var / 2) + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sampling layer\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullData = np.asarray(dataNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"model_17\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_14 (InputLayer)           [(None, 16634)]      0                                            \n__________________________________________________________________________________________________\ndense_79 (Dense)                (None, 8000)         133080000   input_14[0][0]                   \n__________________________________________________________________________________________________\ndense_80 (Dense)                (None, 4000)         32004000    dense_79[0][0]                   \n__________________________________________________________________________________________________\ndense_81 (Dense)                (None, 1000)         4001000     dense_80[0][0]                   \n__________________________________________________________________________________________________\ndense_82 (Dense)                (None, 256)          256256      dense_81[0][0]                   \n__________________________________________________________________________________________________\ndense_83 (Dense)                (None, 32)           8224        dense_82[0][0]                   \n__________________________________________________________________________________________________\ndense_84 (Dense)                (None, 32)           8224        dense_82[0][0]                   \n__________________________________________________________________________________________________\nsampling_6 (Sampling)           (None, 32)           0           dense_83[0][0]                   \n                                                                 dense_84[0][0]                   \n==================================================================================================\nTotal params: 169,357,704\nTrainable params: 169,357,704\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
    }
   ],
   "source": [
    "codings_size = 32\n",
    "\n",
    "inputs = layers.Input(shape=16634)\n",
    "z = layers.Dense(8000)(inputs)\n",
    "z = layers.Dense(4000, activation= \"relu\")(z)\n",
    "z = layers.Dense(1000, activation=\"relu\")(z)\n",
    "z = layers.Dense(256, activation=\"relu\")(z) \n",
    "\n",
    "codings_mean = layers.Dense(codings_size)(z) # μ \n",
    "codings_log_var = layers.Dense(codings_size)(z) # γ \n",
    "codings = Sampling()([codings_mean, codings_log_var]) \n",
    "\n",
    "variational_encoder = Model(\n",
    "    inputs=[inputs], outputs=[codings_mean, codings_log_var, codings])\n",
    "variational_encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"model_18\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_15 (InputLayer)        [(None, 32)]              0         \n_________________________________________________________________\ndense_85 (Dense)             (None, 256)               8448      \n_________________________________________________________________\ndense_86 (Dense)             (None, 1000)              257000    \n_________________________________________________________________\ndense_87 (Dense)             (None, 4000)              4004000   \n_________________________________________________________________\ndense_88 (Dense)             (None, 8000)              32008000  \n_________________________________________________________________\ndense_89 (Dense)             (None, 16634)             133088634 \n=================================================================\nTotal params: 169,366,082\nTrainable params: 169,366,082\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "decoder_inputs = layers.Input(shape=[codings_size])\n",
    "\n",
    "x = layers.Dense(256, activation=\"relu\")(decoder_inputs)\n",
    "x = layers.Dense(1000, activation=\"relu\")(x)\n",
    "x = layers.Dense(4000, activation=\"relu\")(x)\n",
    "x = layers.Dense(8000, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(16634, activation= \"relu\")(x)\n",
    "\n",
    "variational_decoder = Model(inputs=[decoder_inputs], outputs=[outputs])\n",
    "variational_decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# _, _, codings = variational_encoder(inputs)\n",
    "# reconstructions = variational_decoder(codings)\n",
    "# variational_ae = Model(inputs=[inputs], outputs=[reconstructions])\n",
    "# variational_ae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the VAE as a Model with a custom train_step\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, variational_encoder, variational_decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.variational_encoder = variational_encoder\n",
    "        self.variational_decoder = variational_decoder\n",
    "\n",
    "    def train_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            _, _, codings = variational_encoder(data)\n",
    "            reconstruction = variational_decoder(codings)\n",
    "            \n",
    "            omega = tf.sign(data)  # 0 if 0, 1 if > 0\n",
    "            reconstruction_loss = tf.reduce_mean(tf.multiply(tf.pow( (data - reconstruction), 2), omega))\n",
    "    \n",
    "            kl_loss = 1 + codings_log_var - tf.square(codings_mean) - tf.exp(codings_log_var)\n",
    "            kl_loss = tf.reduce_mean(kl_loss)\n",
    "            kl_loss *= -0.5\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        \n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        tf.config.experimental_run_functions_eagerly(True)\n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"reconstruction_loss\": reconstruction_loss,\n",
    "            \"kl_loss\": kl_loss,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "variational_ae = VAE(variational_encoder, variational_decoder)\n",
    "variational_ae.compile(optimizer= 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "VAEresults = variational_ae.fit(fullData, \n",
    "                                epochs= 10, \n",
    "                                batch_size= 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this approach didn't work (too large of a size). Trying a different approach..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder (again)\n",
    "http://louistiao.me/posts/implementing-variational-autoencoders-in-keras-beyond-the-quickstart-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "fullData = np.asarray(dataNorm)[:,:1000]\n",
    "original_dim = 1000\n",
    "latent_dim = 16\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "epsilon_std = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nzMSE(y_true, y_pred):\n",
    "    \"\"\" MSE for nonzero values. \"\"\"\n",
    "    omega = tf.sign(y_true)  # 0 if 0, 1 if > 0\n",
    "    mse_nz = tf.reduce_mean(tf.multiply(tf.pow( (y_pred - y_true), 2), omega))\n",
    "    return mse_nz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KLDivergenceLayer(layers.Layer):\n",
    "\n",
    "    \"\"\" Identity transform layer that adds KL divergence\n",
    "    to the final model loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(KLDivergenceLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        mu, log_var = inputs\n",
    "\n",
    "        kl_batch = - .5 * K.sum(1 + log_var -\n",
    "                                K.square(mu) -\n",
    "                                K.exp(log_var), axis=-1)\n",
    "\n",
    "        self.add_loss(K.mean(kl_batch), inputs=inputs)\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 128)               2176      \n_________________________________________________________________\ndense_1 (Dense)              (None, 512)               66048     \n_________________________________________________________________\ndense_2 (Dense)              (None, 1000)              513000    \n=================================================================\nTotal params: 581,224\nTrainable params: 581,224\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "# Creating the Decoder \n",
    "decoder = Sequential([\n",
    "    layers.Dense(128, input_dim=latent_dim, activation='relu'),\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dense(original_dim, activation= \"relu\")\n",
    "])\n",
    "\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = layers.Input(shape= (original_dim,))\n",
    "xh = layers.Dense(512, activation=\"relu\")(x)\n",
    "h = layers.Dense(128, activation=\"relu\")(xh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mu = layers.Dense(latent_dim)(h)\n",
    "z_log_var = layers.Dense(latent_dim)(h)\n",
    "\n",
    "z_mu, z_log_var = KLDivergenceLayer()([z_mu, z_log_var])\n",
    "z_sigma = layers.Lambda(lambda t: K.exp(.5*t))(z_log_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = layers.Input(tensor=K.random_normal(stddev=epsilon_std,\n",
    "                                          shape=(K.shape(x)[0], latent_dim)))\n",
    "z_eps = layers.Multiply()([z_sigma, eps])\n",
    "z = layers.Add()([z_mu, z_eps])\n",
    "\n",
    "x_pred = decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_3 (InputLayer)            [(None, 1000)]       0                                            \n__________________________________________________________________________________________________\ndense_5 (Dense)                 (None, 512)          512512      input_3[0][0]                    \n__________________________________________________________________________________________________\ndense_6 (Dense)                 (None, 128)          65664       dense_5[0][0]                    \n__________________________________________________________________________________________________\ndense_7 (Dense)                 (None, 16)           2064        dense_6[0][0]                    \n__________________________________________________________________________________________________\ndense_8 (Dense)                 (None, 16)           2064        dense_6[0][0]                    \n__________________________________________________________________________________________________\nkl_divergence_layer (KLDivergen [(None, 16), (None,  0           dense_7[0][0]                    \n                                                                 dense_8[0][0]                    \n__________________________________________________________________________________________________\nlambda (Lambda)                 (None, 16)           0           kl_divergence_layer[0][1]        \n__________________________________________________________________________________________________\ninput_4 (InputLayer)            [(None, 16)]         0                                            \n__________________________________________________________________________________________________\nmultiply_1 (Multiply)           (None, 16)           0           lambda[0][0]                     \n                                                                 input_4[0][0]                    \n__________________________________________________________________________________________________\nadd (Add)                       (None, 16)           0           kl_divergence_layer[0][0]        \n                                                                 multiply_1[0][0]                 \n__________________________________________________________________________________________________\nsequential (Sequential)         (None, 1000)         581224      add[0][0]                        \n==================================================================================================\nTotal params: 1,163,528\nTrainable params: 1,163,528\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
    }
   ],
   "source": [
    "vae = Model(inputs=[x, eps], outputs=x_pred)\n",
    "vae.compile(optimizer= 'adam', loss= nzMSE)\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/10\n"
    },
    {
     "output_type": "error",
     "ename": "AssertionError",
     "evalue": "in user code:\n\n    /home/ahmadazim/anaconda3/envs/tflow/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    /home/ahmadazim/anaconda3/envs/tflow/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/ahmadazim/anaconda3/envs/tflow/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/ahmadazim/anaconda3/envs/tflow/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/ahmadazim/anaconda3/envs/tflow/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:531 train_step  **\n        y_pred = self(x, training=True)\n    /home/ahmadazim/anaconda3/envs/tflow/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:927 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    /home/ahmadazim/anaconda3/envs/tflow/lib/python3.8/site-packages/tensorflow/python/keras/engine/network.py:717 call\n        return self._run_internal_graph(\n    /home/ahmadazim/anaconda3/envs/tflow/lib/python3.8/site-packages/tensorflow/python/keras/engine/network.py:899 _run_internal_graph\n        assert str(id(x)) in tensor_dict, 'Could not compute output ' + str(x)\n\n    AssertionError: Could not compute output Tensor(\"sequential/Identity:0\", shape=(None, 1000), dtype=float32)\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-03db0fc23601>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m vae.fit(fullData,\n\u001b[0m\u001b[1;32m      2\u001b[0m         \u001b[0mfullData\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         batch_size=batch_size)\n",
      "\u001b[0;32m~/anaconda3/envs/tflow/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tflow/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    616\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/anaconda3/envs/tflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2417\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2419\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2420\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2772\u001b[0m           \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2773\u001b[0m           and call_context_key in self._function_cache.missed):\n\u001b[0;32m-> 2774\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_define_function_with_shape_relaxation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2703\u001b[0m     self._function_cache.arg_relaxed_shapes[rank_only_cache_key] = (\n\u001b[1;32m   2704\u001b[0m         relaxed_arg_shapes)\n\u001b[0;32m-> 2705\u001b[0;31m     graph_function = self._create_graph_function(\n\u001b[0m\u001b[1;32m   2706\u001b[0m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[1;32m   2707\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tflow/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2655\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 2657\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2659\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tflow/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tflow/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tflow/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: in user code:\n\n    /home/ahmadazim/anaconda3/envs/tflow/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    /home/ahmadazim/anaconda3/envs/tflow/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /home/ahmadazim/anaconda3/envs/tflow/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /home/ahmadazim/anaconda3/envs/tflow/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /home/ahmadazim/anaconda3/envs/tflow/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:531 train_step  **\n        y_pred = self(x, training=True)\n    /home/ahmadazim/anaconda3/envs/tflow/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:927 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    /home/ahmadazim/anaconda3/envs/tflow/lib/python3.8/site-packages/tensorflow/python/keras/engine/network.py:717 call\n        return self._run_internal_graph(\n    /home/ahmadazim/anaconda3/envs/tflow/lib/python3.8/site-packages/tensorflow/python/keras/engine/network.py:899 _run_internal_graph\n        assert str(id(x)) in tensor_dict, 'Could not compute output ' + str(x)\n\n    AssertionError: Could not compute output Tensor(\"sequential/Identity:0\", shape=(None, 1000), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "vae.fit(fullData,\n",
    "        fullData,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596624839909",
   "display_name": "Python 3.8.3 64-bit ('tflow': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}